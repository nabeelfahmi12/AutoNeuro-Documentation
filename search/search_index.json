{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"License/","text":"LICENSE \u00b6 The MIT License (MIT) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"License/#license","text":"The MIT License (MIT) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"LICENSE"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/","text":"Contributing to AutoNeuro \u00b6 Bug Reporting \u00b6 Your code doesn't work ?? Follow these steps to report a bug. Your bug may already be fixed. Make sure to update to the current version. Search for similar issues among Github issues. It's possible somebody has encountered this bug already. Also remember to check out our FAQ. Still having a problem? Please write to us @com Make sure you provide us with useful information about your configuration. The more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial. Feature Request \u00b6 Provide a clear and detailed explanation of the feature you want why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library. It is crucial for Autoneuro to avoid bloating the API and codebase. Provide code snippets demonstrating the API you have in mind illustrate the use cases of your feature. Change proposal(API or User interface) \u00b6 If you interested in adding a new API , please go through the developer's guide to understand the package in detail Examples \u00b6","title":"Help us improve"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#contributing-to-autoneuro","text":"","title":"Contributing to AutoNeuro"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#bug-reporting","text":"Your code doesn't work ?? Follow these steps to report a bug. Your bug may already be fixed. Make sure to update to the current version. Search for similar issues among Github issues. It's possible somebody has encountered this bug already. Also remember to check out our FAQ. Still having a problem? Please write to us @com Make sure you provide us with useful information about your configuration. The more information you provide, the easier it is for us to validate that there is a bug and the faster we'll be able to take action. If you want your issue to be resolved quickly, following the steps above is crucial.","title":"Bug Reporting"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#feature-request","text":"Provide a clear and detailed explanation of the feature you want why it's important to add. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on library. It is crucial for Autoneuro to avoid bloating the API and codebase. Provide code snippets demonstrating the API you have in mind illustrate the use cases of your feature.","title":"Feature Request"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#change-proposalapi-or-user-interface","text":"If you interested in adding a new API , please go through the developer's guide to understand the package in detail","title":"Change proposal(API or User interface)"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#examples","text":"","title":"Examples"},{"location":"ForDevelopers/Applicationworkflow/","text":"Application Workflow \u00b6 Exception Scenarios \u00b6 Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message User gives wrong null symbol Give proper error message Ask the user to provide correct symbol used for missing values If the cluster contains only one class No error message required Handle this exception internally. User doesn\u2019t know. Deployment credentials are wrong Give proper error message Ask for the details to be entered again","title":"Workflow"},{"location":"ForDevelopers/Applicationworkflow/#application-workflow","text":"","title":"Application Workflow"},{"location":"ForDevelopers/Applicationworkflow/#exception-scenarios","text":"Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message User gives wrong null symbol Give proper error message Ask the user to provide correct symbol used for missing values If the cluster contains only one class No error message required Handle this exception internally. User doesn\u2019t know. Deployment credentials are wrong Give proper error message Ask for the details to be entered again","title":"Exception Scenarios"},{"location":"ForDevelopers/MethodsforModelbuilding/","text":"Model Selection \u00b6 Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. It is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.) and across models of the same type configured with different model hyperparameters (e.g. different kernels in an SVM). Exception Scenarios \u00b6 Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input Model Tuning and Optimization \u00b6 Divide into train and test \u00b6 The data should have been divided into train and validation set before this. Methods for hyper tuning all kinds of models. Regression: \u00b6 Linear Regression \u00b6 Linear Regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). KNN Regression Model Polynomial Regression \u00b6 Polynomial Regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x Model selection criteria: \u00b6 MSE \u00b6 The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. RMSE \u00b6 Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. R squared \u00b6 R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. adjusted R squared \u00b6 The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. Classification: \u00b6 Classification is an important and popular machine learning tool that assigns items in a data set to different categories. Classification is used to predict risk over time, in fraud detection, text categorization, and more. Classification functions begin with a data set where the different categories are known. For example, suppose you want to classify students based on how likely they are to get into graduate school. In addition to factors like admission score exams and grades, you could also track work experience. Logistic Regression \u00b6 Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Decision Tree \u00b6 A decision tree is a flowchart-like structure in which each internal node represents a \u201ctest\u201d on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes) Random Forest \u00b6 Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[ XG Boost \u00b6 XGBoost is an implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning. Support Vector Classifier \u00b6 In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. The Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. KNN Classifier \u00b6 K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well \u2212 Lazy learning algorithm \u2212 KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classification. Non-parametric learning algorithm \u2212 KNN is also a non-parametric learning algorithm because it doesn\u2019t assume anything about the underlying data. Working of KNN Algorithm K-nearest neighbors (KNN) algorithm uses \u2018feature similarity\u2019 to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps \u2212 Step 1 \u2212 For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data. Step 2 \u2212 Next, we need to choose the value of K i.e. the nearest data points. K can be any integer. Step 3 \u2212 For each point in the test data do the following \u2212 3.1 \u2212 Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean. 3.2 \u2212 Now, based on the distance value, sort them in ascending order. 3.3 \u2212 Next, it will choose the top K rows from the sorted array. 3.4 \u2212 Now, it will assign a class to the test point based on most frequent class of these rows. Na\u00efve Baye\u2019s \u00b6 Na\u00efve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. They are among the simplest Bayesian network models. But they could be coupled with Kernel density estimation and achieve higher accuracy levels. Na\u00efve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. Model selection criteria: \u00b6 Accuracy \u00b6 Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition: Accuracy = Number of correct predictions Total number of predictions. AUC \u00b6 AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. PrecisionRecall \u00b6 Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly. F1 Score \u00b6 F1 Score combines Recall and Precision to one performance metric. F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than Accuracy, especially if you have an uneven class distribution. Clustering: \u00b6 Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields. K-Means \u00b6 k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. Hierarchial \u00b6 Hierarchical clustering , also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other. DBSCAN \u00b6 K-Means clustering may cluster loosely related observations together. Every observation becomes a part of some cluster eventually, even if the observations are scattered far away in the vector space. Since clusters depend on the mean value of cluster elements, each data point plays a role in forming the clusters. A slight change in data points might affect the clustering outcome. This problem is greatly reduced in DBSCAN due to the way clusters are formed. This is usually not a big problem unless we come across some odd shape data. What\u2019s nice about DBSCAN is that you don\u2019t have to specify the number of clusters to use it. All you need is a function to calculate the distance between values and some guidance for what amount of distance is considered \u201cclose\u201d. DBSCAN also produces more reasonable results than k-means across a variety of different distributions Class Name ModelTuner Method Name get_tuned_knn_model Method Description This method will be used to get the hypertuned KNN Model Input parameter names self,data Input Parameter Description Data: the training data Hyperparameters to tune ouptput A hyper parameter tuned model object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation","title":"Model building"},{"location":"ForDevelopers/MethodsforModelbuilding/#model-selection","text":"Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. It is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.) and across models of the same type configured with different model hyperparameters (e.g. different kernels in an SVM).","title":"Model Selection"},{"location":"ForDevelopers/MethodsforModelbuilding/#exception-scenarios","text":"Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input","title":"Exception Scenarios"},{"location":"ForDevelopers/MethodsforModelbuilding/#model-tuning-and-optimization","text":"","title":"Model Tuning and Optimization"},{"location":"ForDevelopers/MethodsforModelbuilding/#divide-into-train-and-test","text":"The data should have been divided into train and validation set before this. Methods for hyper tuning all kinds of models.","title":"Divide into train and test"},{"location":"ForDevelopers/MethodsforModelbuilding/#regression","text":"","title":"Regression:"},{"location":"ForDevelopers/MethodsforModelbuilding/#linear-regression","text":"Linear Regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). KNN Regression Model","title":"Linear Regression"},{"location":"ForDevelopers/MethodsforModelbuilding/#polynomial-regression","text":"Polynomial Regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x","title":"Polynomial Regression"},{"location":"ForDevelopers/MethodsforModelbuilding/#model-selection-criteria","text":"","title":"Model selection criteria:"},{"location":"ForDevelopers/MethodsforModelbuilding/#mse","text":"The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.","title":"MSE"},{"location":"ForDevelopers/MethodsforModelbuilding/#rmse","text":"Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.","title":"RMSE"},{"location":"ForDevelopers/MethodsforModelbuilding/#r-squared","text":"R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.","title":"R squared"},{"location":"ForDevelopers/MethodsforModelbuilding/#adjusted-r-squared","text":"The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.","title":"adjusted R squared"},{"location":"ForDevelopers/MethodsforModelbuilding/#classification","text":"Classification is an important and popular machine learning tool that assigns items in a data set to different categories. Classification is used to predict risk over time, in fraud detection, text categorization, and more. Classification functions begin with a data set where the different categories are known. For example, suppose you want to classify students based on how likely they are to get into graduate school. In addition to factors like admission score exams and grades, you could also track work experience.","title":"Classification:"},{"location":"ForDevelopers/MethodsforModelbuilding/#logistic-regression","text":"Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).","title":"Logistic Regression"},{"location":"ForDevelopers/MethodsforModelbuilding/#decision-tree","text":"A decision tree is a flowchart-like structure in which each internal node represents a \u201ctest\u201d on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes)","title":"Decision Tree"},{"location":"ForDevelopers/MethodsforModelbuilding/#random-forest","text":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[","title":"Random Forest"},{"location":"ForDevelopers/MethodsforModelbuilding/#xg-boost","text":"XGBoost is an implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning.","title":"XG Boost"},{"location":"ForDevelopers/MethodsforModelbuilding/#support-vector-classifier","text":"In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. The Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems.","title":"Support Vector Classifier"},{"location":"ForDevelopers/MethodsforModelbuilding/#knn-classifier","text":"K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well \u2212 Lazy learning algorithm \u2212 KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classification. Non-parametric learning algorithm \u2212 KNN is also a non-parametric learning algorithm because it doesn\u2019t assume anything about the underlying data. Working of KNN Algorithm K-nearest neighbors (KNN) algorithm uses \u2018feature similarity\u2019 to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps \u2212 Step 1 \u2212 For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data. Step 2 \u2212 Next, we need to choose the value of K i.e. the nearest data points. K can be any integer. Step 3 \u2212 For each point in the test data do the following \u2212 3.1 \u2212 Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean. 3.2 \u2212 Now, based on the distance value, sort them in ascending order. 3.3 \u2212 Next, it will choose the top K rows from the sorted array. 3.4 \u2212 Now, it will assign a class to the test point based on most frequent class of these rows.","title":"KNN Classifier"},{"location":"ForDevelopers/MethodsforModelbuilding/#naive-bayes","text":"Na\u00efve Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features. They are among the simplest Bayesian network models. But they could be coupled with Kernel density estimation and achieve higher accuracy levels. Na\u00efve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.","title":"Na\u00efve Baye\u2019s"},{"location":"ForDevelopers/MethodsforModelbuilding/#model-selection-criteria_1","text":"","title":"Model selection criteria:"},{"location":"ForDevelopers/MethodsforModelbuilding/#accuracy","text":"Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition: Accuracy = Number of correct predictions Total number of predictions.","title":"Accuracy"},{"location":"ForDevelopers/MethodsforModelbuilding/#auc","text":"AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.","title":"AUC"},{"location":"ForDevelopers/MethodsforModelbuilding/#precisionrecall","text":"Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.","title":"PrecisionRecall"},{"location":"ForDevelopers/MethodsforModelbuilding/#f1-score","text":"F1 Score combines Recall and Precision to one performance metric. F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than Accuracy, especially if you have an uneven class distribution.","title":"F1 Score"},{"location":"ForDevelopers/MethodsforModelbuilding/#clustering","text":"Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.","title":"Clustering:"},{"location":"ForDevelopers/MethodsforModelbuilding/#k-means","text":"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.","title":"K-Means"},{"location":"ForDevelopers/MethodsforModelbuilding/#hierarchial","text":"Hierarchical clustering , also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.","title":"Hierarchial"},{"location":"ForDevelopers/MethodsforModelbuilding/#dbscan","text":"K-Means clustering may cluster loosely related observations together. Every observation becomes a part of some cluster eventually, even if the observations are scattered far away in the vector space. Since clusters depend on the mean value of cluster elements, each data point plays a role in forming the clusters. A slight change in data points might affect the clustering outcome. This problem is greatly reduced in DBSCAN due to the way clusters are formed. This is usually not a big problem unless we come across some odd shape data. What\u2019s nice about DBSCAN is that you don\u2019t have to specify the number of clusters to use it. All you need is a function to calculate the distance between values and some guidance for what amount of distance is considered \u201cclose\u201d. DBSCAN also produces more reasonable results than k-means across a variety of different distributions Class Name ModelTuner Method Name get_tuned_knn_model Method Description This method will be used to get the hypertuned KNN Model Input parameter names self,data Input Parameter Description Data: the training data Hyperparameters to tune ouptput A hyper parameter tuned model object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation","title":"DBSCAN"},{"location":"ForDevelopers/Methodsfordataprofiler/","text":"Data profiling \u00b6 Data profiling is the process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects. Method Definition Data Profiler \u00b6 Class Name DataProfiler Method Name get_data_profile Method Description This method will be used to give various insighst about data. Input parameter names self, dataframe Input Parameter Description dataframe: the inpt data just loaded from source ouptput a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data profiling"},{"location":"ForDevelopers/Methodsfordataprofiler/#data-profiling","text":"Data profiling is the process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects. Method Definition","title":"Data profiling"},{"location":"ForDevelopers/Methodsfordataprofiler/#data-profiler","text":"Class Name DataProfiler Method Name get_data_profile Method Description This method will be used to give various insighst about data. Input parameter names self, dataframe Input Parameter Description dataframe: the inpt data just loaded from source ouptput a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Profiler"},{"location":"ForDevelopers/Methodsfordataupload/","text":"Data Ingestion and File Conversion \u00b6 Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse, data mart, database, or a document store. Data Connector Utils File Conversion Utils Microsoft Access CSV & text files, PDF Spatial File JSON Statistical File HTML Tableau Server or Tableau Online Excel files Actian Matrix openDocument Spreadsheets Actian Vectorwise Binary Excel (.xlsb) files Alibaba AnalyticDB for MySQL Clipboard Alibaba Data Lake Analytics Pickling Alibaba MaxCompute msgpack Amazon Athena HDF5 (PyTables) Amazon Aurora for MySQL Feather Amazon EMR Hadoop Hive Parquet Amazon Redshift ORC Anaplan Google BigQuery Apache Drill Stata format Aster Database SAS formats Azure SQL Synapse Analytics SPSS formats Box Other file formats Cloudera Hadoop Performance considerations Databricks Denodo Dropbo Esri ArcGIS Server Exasol Firebird 3 Google Ads Google Analytics Google BigQuery Google Cloud SQL Google Drive Google Sheets Hortonworks Hadoop Hive IBM BigInsights IBM DB2 IBM PDA (Netezza) Impala Intuit QuickBooks Online Kognitio Kyvos LinkedIn Sales Navigator MapR Hadoop Hive MariaDB arketo MarkLogic MemSQL Microsoft Analysis Services Microsoft PowerPivot Microsoft SQL Server MonetDB MongoDB BI Connector MySQL OData OneDrive Oracle Oracle Eloqua Oracle Essbase Pivotal Greenplum PostgreSQL Presto Progress OpenEdge Qubole Presto Salesforce Splunk SAP HANA SAP NetWeaver Business Warehouse SAP Sybase ASE SAP Sybase IQ ServiceNow ITSM SharePoint Lists Snowflake Spark SQL Connector Plugin Web Data Connector Other Databases (JDBC) Other Databases (ODBC) Upload Data \u00b6 Read from csv \u00b6 Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from json \u00b6 Method Name read_data_from_json Method Description This method will be used to read data from a json file. Input parameter names self,file_name Input Parameter Description file_name: name of the file to be read ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from html \u00b6 Method Name read_data_from_html Method Description This method will be used to read data from an HTML web page Input parameter names self,url Input Parameter Description url: URL of the HTML page to be read. ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from Excel \u00b6 Method Name read_data_from_excel Method Description This method will be used to read data from an MS Excel File Input parameter names self,file_name,sheet_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read sheet_name: Lists of strings/integers are used to request multiple sheets. Specify None to get all sheets. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Connecting to sqldb \u00b6 Method Name Connect_to_sqldb Method Description This method will be used to connect to a SQL Databases Input parameter names self,host,port, username, password Input Parameter Description host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server ouptput A DB connection object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from sqldb \u00b6 Method Name read_data_from_sqldb Method Description This method will be used to read data from SQL Databases Input parameter names self,db_name,host,port, username, password, schema_name,query_string Input Parameter Description db_name: For example, SQL, MySQL, SQLLite etc. host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server schema_name: The name of the DB schema the user wants to connect to. query_string: the query to be executed to load the data ouptput A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from mongodb \u00b6 Method Name read_data_from_mongdb Method Description This method will be used to read data from Mongo DB Input parameter names self,host,port, username, password, db_name,collection_name, query_string. Input Parameter Description output A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios \u00b6 Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message","title":"Data upload"},{"location":"ForDevelopers/Methodsfordataupload/#data-ingestion-and-file-conversion","text":"Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse, data mart, database, or a document store. Data Connector Utils File Conversion Utils Microsoft Access CSV & text files, PDF Spatial File JSON Statistical File HTML Tableau Server or Tableau Online Excel files Actian Matrix openDocument Spreadsheets Actian Vectorwise Binary Excel (.xlsb) files Alibaba AnalyticDB for MySQL Clipboard Alibaba Data Lake Analytics Pickling Alibaba MaxCompute msgpack Amazon Athena HDF5 (PyTables) Amazon Aurora for MySQL Feather Amazon EMR Hadoop Hive Parquet Amazon Redshift ORC Anaplan Google BigQuery Apache Drill Stata format Aster Database SAS formats Azure SQL Synapse Analytics SPSS formats Box Other file formats Cloudera Hadoop Performance considerations Databricks Denodo Dropbo Esri ArcGIS Server Exasol Firebird 3 Google Ads Google Analytics Google BigQuery Google Cloud SQL Google Drive Google Sheets Hortonworks Hadoop Hive IBM BigInsights IBM DB2 IBM PDA (Netezza) Impala Intuit QuickBooks Online Kognitio Kyvos LinkedIn Sales Navigator MapR Hadoop Hive MariaDB arketo MarkLogic MemSQL Microsoft Analysis Services Microsoft PowerPivot Microsoft SQL Server MonetDB MongoDB BI Connector MySQL OData OneDrive Oracle Oracle Eloqua Oracle Essbase Pivotal Greenplum PostgreSQL Presto Progress OpenEdge Qubole Presto Salesforce Splunk SAP HANA SAP NetWeaver Business Warehouse SAP Sybase ASE SAP Sybase IQ ServiceNow ITSM SharePoint Lists Snowflake Spark SQL Connector Plugin Web Data Connector Other Databases (JDBC) Other Databases (ODBC)","title":"Data Ingestion and File Conversion"},{"location":"ForDevelopers/Methodsfordataupload/#upload-data","text":"","title":"Upload Data"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-csv","text":"Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from csv"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-json","text":"Method Name read_data_from_json Method Description This method will be used to read data from a json file. Input parameter names self,file_name Input Parameter Description file_name: name of the file to be read ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from json"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-html","text":"Method Name read_data_from_html Method Description This method will be used to read data from an HTML web page Input parameter names self,url Input Parameter Description url: URL of the HTML page to be read. ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from html"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-excel","text":"Method Name read_data_from_excel Method Description This method will be used to read data from an MS Excel File Input parameter names self,file_name,sheet_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read sheet_name: Lists of strings/integers are used to request multiple sheets. Specify None to get all sheets. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from Excel"},{"location":"ForDevelopers/Methodsfordataupload/#connecting-to-sqldb","text":"Method Name Connect_to_sqldb Method Description This method will be used to connect to a SQL Databases Input parameter names self,host,port, username, password Input Parameter Description host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server ouptput A DB connection object On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Connecting to sqldb"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-sqldb","text":"Method Name read_data_from_sqldb Method Description This method will be used to read data from SQL Databases Input parameter names self,db_name,host,port, username, password, schema_name,query_string Input Parameter Description db_name: For example, SQL, MySQL, SQLLite etc. host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server schema_name: The name of the DB schema the user wants to connect to. query_string: the query to be executed to load the data ouptput A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from sqldb"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-mongodb","text":"Method Name read_data_from_mongdb Method Description This method will be used to read data from Mongo DB Input parameter names self,host,port, username, password, db_name,collection_name, query_string. Input Parameter Description output A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from mongodb"},{"location":"ForDevelopers/Methodsfordataupload/#exceptions-scenarios","text":"Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message","title":"Exceptions Scenarios"},{"location":"ForDevelopers/Methodsfordeployment/","text":"Deployment Strategy \u00b6 Take the cloud name as input Prepare the metadata files based on cloud Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging","title":"Deployment"},{"location":"ForDevelopers/Methodsfordeployment/#deployment-strategy","text":"Take the cloud name as input Prepare the metadata files based on cloud Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging","title":"Deployment Strategy"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/","text":"Once the pandas dataframe is given, using data visualization methods we will get corresponding data in the form of graph. Correlation Heatmaps \u00b6 Check for balance/imbalance \u00b6 Data Visualisation \u00b6 Class Name DataVisualization Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise \u00b6 Step Exception Mitigation Wrong input to the methods Handle Internally Code should never give a wrong input","title":"Data Visualization(Graphical Analysis)"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#correlation-heatmaps","text":"","title":"Correlation Heatmaps"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#check-for-balanceimbalance","text":"","title":"Check for balance/imbalance"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#data-visualisation","text":"Class Name DataVisualization Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Visualisation"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#exceptions-scenarios-module-wise","text":"Step Exception Mitigation Wrong input to the methods Handle Internally Code should never give a wrong input","title":"Exceptions Scenarios Module Wise"},{"location":"ForDevelopers/Methodsforlogging/","text":"Separate Folder for logs Logging of every step Entry to the methods Exit from the methods with success/ failure message Error message Logging Model comparisons Training start and end Prediction start and end Achieve asynchronous logging Class Name App Logger Method Name log Method Description This method will be used for logging all the information to the file. Input parameter names self,file_object, log_message Input Parameter Description file_object: the file where the logs will be written log_message: the message to be logged ouptput A log file with messages from datetime import datetime class App_Logger : def __init__ ( self ): pass def log ( self , file_object , log_message ): \u201c\u201d\u201d This method will be used for logging all the information to the file . \u201d\u201d\u201d self . now = datetime . now () self . date = self . now . date () self . current_time = self . now . strftime ( \"%H:%M:%S\" ) file_object . write ( str ( self . date ) + \"/\" + str ( self . current_time ) + \" \\t\\t \" + log_message + \" \\n \" ) Exceptions Scenarios Module Wise \u00b6 Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging","title":"Logging"},{"location":"ForDevelopers/Methodsforlogging/#exceptions-scenarios-module-wise","text":"Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging","title":"Exceptions Scenarios Module Wise"},{"location":"ForDevelopers/Methodsforprediction/","text":"Testing Modules \u00b6 It Divide the training data itself into train and test sets Use test data to have tests run on the three best models Give the test report_: - R2 Score - Adjusted R2 score - MSE - Accuracy - Precision - Recall - F Beta - Cluster Purity - Silhouette score Step Exception Mitigation Number of Parameters do not match Handle internally Check the test data creation and verify the columns Only once class present in test data Handle Internally","title":"Model Testing & Selection"},{"location":"ForDevelopers/Methodsforprediction/#testing-modules","text":"It Divide the training data itself into train and test sets Use test data to have tests run on the three best models Give the test report_: - R2 Score - Adjusted R2 score - MSE - Accuracy - Precision - Recall - F Beta - Cluster Purity - Silhouette score Step Exception Mitigation Number of Parameters do not match Handle internally Check the test data creation and verify the columns Only once class present in test data Handle Internally","title":"Testing Modules"},{"location":"ForDevelopers/Methodsforpreprocessing/","text":"Transformations \u00b6 Imputing missing values \u00b6 Method Name: impute_missing_values Description: This method will be used to impute missing values in the dataframe Categorical to numerical \u00b6 Method Name: type_conversion Description: This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible. Imbalanced data set handling \u00b6 Method Name: remove_imbalance Description: This method will be used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques Handling columns with std deviation zero or below a threshold \u00b6 Method Name: standardize_data Description: This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1. Normalisation \u00b6 Method Name: normalize_data Description: This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1. PCA \u00b6 Method Name: pca Description: This method reduces the dimension from scaled Data which enables quick for large data files. Data Preprocessor \u00b6 Class Name DataPreprocessor Method Name impute_missing_values Method Description This method will be used to read data from a csv file or a flat file. Input parameter names self,file_name, header,names, use_cols, separator. Input Parameter Description file_name: name of the file to be read. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use Output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise \u00b6 Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input","title":"Data preprocessing"},{"location":"ForDevelopers/Methodsforpreprocessing/#transformations","text":"","title":"Transformations"},{"location":"ForDevelopers/Methodsforpreprocessing/#imputing-missing-values","text":"Method Name: impute_missing_values Description: This method will be used to impute missing values in the dataframe","title":"Imputing missing values"},{"location":"ForDevelopers/Methodsforpreprocessing/#categorical-to-numerical","text":"Method Name: type_conversion Description: This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible.","title":"Categorical to numerical"},{"location":"ForDevelopers/Methodsforpreprocessing/#imbalanced-data-set-handling","text":"Method Name: remove_imbalance Description: This method will be used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques","title":"Imbalanced data set handling"},{"location":"ForDevelopers/Methodsforpreprocessing/#handling-columns-with-std-deviation-zero-or-below-a-threshold","text":"Method Name: standardize_data Description: This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1.","title":"Handling columns with std deviation zero or below a threshold"},{"location":"ForDevelopers/Methodsforpreprocessing/#normalisation","text":"Method Name: normalize_data Description: This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1.","title":"Normalisation"},{"location":"ForDevelopers/Methodsforpreprocessing/#pca","text":"Method Name: pca Description: This method reduces the dimension from scaled Data which enables quick for large data files.","title":"PCA"},{"location":"ForDevelopers/Methodsforpreprocessing/#data-preprocessor","text":"Class Name DataPreprocessor Method Name impute_missing_values Method Description This method will be used to read data from a csv file or a flat file. Input parameter names self,file_name, header,names, use_cols, separator. Input Parameter Description file_name: name of the file to be read. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use Output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Preprocessor"},{"location":"ForDevelopers/Methodsforpreprocessing/#exceptions-scenarios-module-wise","text":"Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input","title":"Exceptions Scenarios Module Wise"},{"location":"ForDevelopers/MethodsforstatbasedEDA/","text":"Technical solution \u00b6 OLS \u00b6 Ordinary least squares The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values. VIF \u00b6 The variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. Correlation: Correlation is a statistical technique that can show whether and how strongly pairs of variables are related Anova test \u00b6 An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis. Basically, you're testing groups to see if there's a difference between them. Chi-square test \u00b6 Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. Z-test \u00b6 A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test. T test \u00b6 The t test is one type of inferential statistics. It is used to determine whether there is a significant difference between the means of two groups. With all inferential statistics, we assume the dependent variable fits a normal distribution Weight of evidence \u00b6 The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. F-test \u00b6 An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exceptions Scenarios \u00b6 Step Exception Mitigation Column has mixed values(Integer & number) Give proper error message Ask the user to correct the data. Not all values are numbers Handle Internally Convert categorical to numerical values Seasonality: \u00b6 Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal. Stationary Data: \u00b6 A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time","title":"Data Visualization(Statistical Analysis)"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#technical-solution","text":"","title":"Technical solution"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#ols","text":"Ordinary least squares The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values.","title":"OLS"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#vif","text":"The variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. Correlation: Correlation is a statistical technique that can show whether and how strongly pairs of variables are related","title":"VIF"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#anova-test","text":"An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis. Basically, you're testing groups to see if there's a difference between them.","title":"Anova test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#chi-square-test","text":"Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table.","title":"Chi-square test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#z-test","text":"A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test.","title":"Z-test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#t-test","text":"The t test is one type of inferential statistics. It is used to determine whether there is a significant difference between the means of two groups. With all inferential statistics, we assume the dependent variable fits a normal distribution","title":"T test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#weight-of-evidence","text":"The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable.","title":"Weight of evidence"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#f-test","text":"An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled.","title":"F-test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#exceptions-scenarios","text":"Step Exception Mitigation Column has mixed values(Integer & number) Give proper error message Ask the user to correct the data. Not all values are numbers Handle Internally Convert categorical to numerical values","title":"Exceptions Scenarios"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#seasonality","text":"Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal.","title":"Seasonality:"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#stationary-data","text":"A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time","title":"Stationary Data:"},{"location":"ForDevelopers/Api/API_predict/","text":"Data Visualization: \u00b6 This class shall be used to include all Data Visualization techniques to be feed to the Machine Learning Models Balance imbalance check \u00b6 This method will be used to plot the balance/imbalance datasets using barplot/countplot Input: data: the input dataframe with target column. Target: target column name. Output: plot of target variable value count. def balance_imbalance_check ( self , dataframe , target ): sns . barplot ( x = 'is_promoted' , y = 'is_promoted' , data = dataframe , estimator = lambda x : len ( x ) / len ( dataframe ) * 100 ) plt . xlabel ( 'ispromoted' ) plt . ylabel ( 'percentage' ) plt . title ( 'Balance Imbalance Count' ) plt . savefig ( \"static/graphs/imbalance.png\" ) Correlation Heatmap \u00b6 This method will be used to generate interactive heatmap plot to show the pairwise correlation of input variables Input Description: data: the input dataframe with target column. Output: returns json file with correlation information that can be used by plotly to generate interactive plots. def correlation_heatmap ( self , dataframe ): data = dataframe . select_dtypes ( include = [ np . number ]) plotdata = [ go . Heatmap ( z = data . corr (), x = list ( data . columns ), y = list ( data . columns ) , dx = 1 , dy = 1 )] graphJSON = json . dumps ( plotdata , cls = plotly . utils . PlotlyJSONEncoder ) return graphJSON","title":"Predict"},{"location":"ForDevelopers/Api/API_predict/#data-visualization","text":"This class shall be used to include all Data Visualization techniques to be feed to the Machine Learning Models","title":"Data Visualization:"},{"location":"ForDevelopers/Api/API_predict/#balance-imbalance-check","text":"This method will be used to plot the balance/imbalance datasets using barplot/countplot Input: data: the input dataframe with target column. Target: target column name. Output: plot of target variable value count. def balance_imbalance_check ( self , dataframe , target ): sns . barplot ( x = 'is_promoted' , y = 'is_promoted' , data = dataframe , estimator = lambda x : len ( x ) / len ( dataframe ) * 100 ) plt . xlabel ( 'ispromoted' ) plt . ylabel ( 'percentage' ) plt . title ( 'Balance Imbalance Count' ) plt . savefig ( \"static/graphs/imbalance.png\" )","title":"Balance imbalance check"},{"location":"ForDevelopers/Api/API_predict/#correlation-heatmap","text":"This method will be used to generate interactive heatmap plot to show the pairwise correlation of input variables Input Description: data: the input dataframe with target column. Output: returns json file with correlation information that can be used by plotly to generate interactive plots. def correlation_heatmap ( self , dataframe ): data = dataframe . select_dtypes ( include = [ np . number ]) plotdata = [ go . Heatmap ( z = data . corr (), x = list ( data . columns ), y = list ( data . columns ) , dx = 1 , dy = 1 )] graphJSON = json . dumps ( plotdata , cls = plotly . utils . PlotlyJSONEncoder ) return graphJSON","title":"Correlation Heatmap"},{"location":"ForDevelopers/Api/API_preprocess/","text":"Preprocessor(): \u00b6 This class shall be used to include all Data Pre-processing techniques to be fed to the Machine Learning Models get_data_profile \u00b6 def get_data_profile ( self , data ): self . data_profile = {} self . missing_values = {} self . missing_val_pct = {} self . data_profile [ 'rows' ] = data . shape [ 0 ] self . data_profile [ 'columns' ] = data . shape [ 1 ] self . missing_vals = data . isna () . sum () for col in data . columns : if data [ col ] . isnull () . sum () > 0 : self . missing_values [ col ] = data [ col ] . isnull () . sum () self . missing_val_pct [ col ] = ( data [ col ] . isnull () . sum () / len ( data )) * 100 self . data_profile [ 'missing_values' ] = self . missing_values self . data_profile [ 'missing_vals_pct' ] = self . missing_val_pct self . data_profile [ 'categorical_columns' ] = list ( data . select_dtypes ( exclude = [ \"int64\" , \"float\" ])) self . data_profile [ 'num_categorical_columns' ] = len ( self . data_profile [ 'categorical_columns' ]) self . data_profile [ 'numerical_columns' ] = list ( data . select_dtypes ( exclude = [ \"object\" ])) self . data_profile [ 'num_numerical_columns' ] = len ( self . data_profile [ 'numerical_columns' ]) self . data_profile [ 'num_duplicate_rows' ] = data . duplicated () . sum () self . describe = data . describe () . T self . standard_deviation = self . describe [ self . describe [ 'std' ] == 0 ] self . standard_deviation = list ( self . standard_deviation . index ) self . data_profile [ 'num_col_with_std_zero' ] = len ( self . standard_deviation ) if len ( self . standard_deviation ) > 0 : self . data_profile [ 'cols_with_std_zero' ] = self . standard_deviation self . size = data . size / ( 1024 * 1024 ) self . data_profile [ 'datasize' ] = str ( round ( self . size , 2 )) + \" MB\" return self . data_profile separatete Label features \u00b6 This is used to separate the target and feature as X and Y def separate_label_feature ( self , data , label_column_name ): self . X = data . drop ( labels = label_column_name , axis = 1 ) # drop the columns specified and separate the feature columns self . Y = data [ label_column_name ] # Filter the Label columns return self . X , self . Y remove_columns \u00b6 This method removes the given columns from a pandas dataframe. def remove_columns ( self , data , columns ): self . data = data self . columns = columns self . useful_data = self . data . drop ( labels = self . columns , axis = 1 ) # drop the labels specified in the columns return self . useful_data impute_missing_values \u00b6 This method will be used to impute missing values in the dataframe def impute_missing_values ( self , data , mv_flag = None , target = None , strategy = 'mode' , impute_val = None , missing_vals = None ): dataframe = data if mv_flag is True : # Converting missing_vals to Nan Values if missing_vals : dataframe . replace ( missing_vals , np . nan , inplace = True ) # Checking for Missing Values in Dependent Variable if dataframe [ target ] . isna () . any (): dataframe = dataframe [ dataframe [ target ] . notna ()] . copy () # Selecting the Dataframe With No missing values in Dependent column # Checking for Missing Values in Independent Variables Missing_data_columns = dataframe . columns [ dataframe . isna () . any ()] . tolist () # Finding Columns with the missing data from dataframe if strategy == 'fixed' : # checking if strategy == fixed dataframe . fillna ( impute_val , inplace = True ) # Filling the Nan values with the imputed value from user else : for columns in Missing_data_columns : # Iterating over the columns having Nan Values if dataframe [ columns ] . dtype == 'object' : # Checking for the categorical data mode = dataframe [ columns ] . mode ()[ 0 ] dataframe [ columns ] . fillna ( mode , inplace = True ) # Imputing Nan values with mode of the column else : if strategy == 'median' : # checking if the strategy == median median = dataframe [ columns ] . median () dataframe [ columns ] . fillna ( median , inplace = True ) # Imputing Nan values with median of the column else : # The only strategy remains is mean mean = dataframe [ columns ] . mean () dataframe [ columns ] . fillna ( mean , inplace = True ) # Imputing Nan values with mean of the column else : self . logger_object . log ( self . file_object , \"my_flag found False\" ) return dataframe type_conversion \u00b6 This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible. def type_conversion ( self , dataset , cat_to_num = None , num_to_cat = None ): if cat_to_num is not None : for column in cat_to_num : dataset [ column ] = pd . to_numeric ( dataset [ column ]) if num_to_cat is not None : for column in num_to_cat : dataset [ column ] = dataset [ column ] . astype ( 'object' ) return dataset remove_imbalance \u00b6 Used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques Input Description: data: the input dataframe with target column. threshold: the threshold of mismatch between the target values to perform balancing. Output: A balanced dataframe. def remove_imbalance ( self , data , target , threshold = 10.0 , oversample = True , smote = False ): X = data . drop ( target , axis = 1 ) y = data [ target ] self . logger_object . log ( self . file_object , 'Class Imbalance Process Starts in the remove_imbalance method of the DataPreprocessor class' ) no_of_classes = data [ target ] . nunique () if no_of_classes == 2 : thresh_satisfied = (( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . any () < threshold ) if thresh_satisfied : if smote : smote = SMOTE () X , y = smote . fit_resample ( X , y ) elif oversample : ROS = RandomOverSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : ROS = RandomUnderSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : high = ( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . ravel () . max () low = ( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . ravel () . min () thresh_satisfied = ( high - low > 100.0 - threshold ) if thresh_satisfied : if smote : for i in range ( no_of_classes - 2 ): smote = SMOTE () X , y = smote . fit_resample ( X , y ) elif oversample : for i in range ( no_of_classes - 2 ): ROS = RandomOverSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : for i in range ( no_of_classes - 2 ): ROS = RandomUnderSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) y . to_frame ( name = target ) dfBalanced = pd . concat ([ X , y ], axis = 1 ) return dfBalanced remove_columns_with_minimal_variance \u00b6 This method drops any numerical column with standard deviation below specified threshold Input : data: input DataFrame in which we need to check std deviations : threshold : the threshold for std deviation below which we need to drop the columns Output: A DataFrame with numerical columns with low std dev dropped. def remove_columns_with_minimal_variance ( self , data , threshold ): sel = VarianceThreshold ( threshold = ( threshold * ( 1 - threshold ))) columnlist = list ( data . select_dtypes ( include = 'number' ) . columns ) sel_var = sel . fit_transform ( data [ columnlist ]) new_data = data [ data . columns [ sel . get_support ( indices = True )]] return new_data # return the read data to the calling method standardize_data \u00b6 This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1. * Input : the input dataframe with numeric columns. * Output: Standardized data where mean of each column will be 0 and standard deviation will be 1. def standardize_data ( self , dataframe ): data = dataframe stdscalar = StandardScaler () scaled_data = stdscalar . fit_transform ( data ) scaled_data = pd . Dataframe ( data = scaled_data , columns = data . columns ) return scaled_data normalize_data \u00b6 This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1. * Input : the input dataframe with numeric columns. * Output: Normalized data where minimum value of each column will be 0 and maximum value of each column will be 1. def normalize_data ( self , dataframe ): data = dataframe normalizer = MinMaxScaler () normalized_data = normalizer . fit_transform ( data ) normalized_data = pd . Dataframe ( data = normalized_data , columns = data . columns ) return normalized_data pca \u00b6 This method reduces the dimension from scaled Data which enables quick for large data files. * input : Data which is Scaled, var_explained = 0.90(default value) * Output : It returns the scaled and reduced dimensions. def pca ( self , data , var_explained ): self . data = data self . var_explained = var_explained n = len ( data . keys ()) # find out the no columns in the data mat_pca = PCA ( n_components = n ) mat_pca . fit ( data ) # applying PCA model ##calculate variance ratios variance = mat_pca . explained_variance_ratio_ cum_var = np . cumsum ( np . round ( mat_pca . explained_variance_ratio_ , decimals = 3 ) * 100 ) calc_num_components \u00b6 This function is used for calculating number of principal components to use: def calc_num_components ( cum_var , var_explained ): for i in range ( n ): if cum_var [ i ] >= var_explained : return i + 1 # call the function to calulate num_components: n_components = calc_num_components ( cum_var , var_explained ) # create the PCA instance pca = PCA ( n_components = n_components ) principal_components = pca . fit_transform ( data ) # Convert into dataframe pca_data = pd . DataFrame ( data = principal_components , columns = [ 'PC' + str ( i ) for i in range ( 1 , n_components + 1 )]) return pca_data preprocess \u00b6 This function is used to preprocess the data by calling the apis listed above * input: dataset, target columns, unwanted columns * output: preprocessed target and feature dataset def preprocess ( self , dataset , target_column , unwanted_cols ): dataset = self . remove_columns ( dataset , columns = [ 'Unnamed: 0' ]) dataset = self . impute_missing_values ( data = dataset , mv_flag = True , target = target_column ) # dataset = self.remove_columns(dataset, unwanted_cols) dataset = self . remove_imbalance ( dataset , target_column , threshold = 10.0 , oversample = True , smote = False ) dataset , self . y = self . separate_label_feature ( dataset , target_column ) self . x = self . remove_columns_with_minimal_variance ( data = dataset , threshold = 0.1 ) return self . x , self . y","title":"Data Preprocess"},{"location":"ForDevelopers/Api/API_preprocess/#preprocessor","text":"This class shall be used to include all Data Pre-processing techniques to be fed to the Machine Learning Models","title":"Preprocessor():"},{"location":"ForDevelopers/Api/API_preprocess/#get_data_profile","text":"def get_data_profile ( self , data ): self . data_profile = {} self . missing_values = {} self . missing_val_pct = {} self . data_profile [ 'rows' ] = data . shape [ 0 ] self . data_profile [ 'columns' ] = data . shape [ 1 ] self . missing_vals = data . isna () . sum () for col in data . columns : if data [ col ] . isnull () . sum () > 0 : self . missing_values [ col ] = data [ col ] . isnull () . sum () self . missing_val_pct [ col ] = ( data [ col ] . isnull () . sum () / len ( data )) * 100 self . data_profile [ 'missing_values' ] = self . missing_values self . data_profile [ 'missing_vals_pct' ] = self . missing_val_pct self . data_profile [ 'categorical_columns' ] = list ( data . select_dtypes ( exclude = [ \"int64\" , \"float\" ])) self . data_profile [ 'num_categorical_columns' ] = len ( self . data_profile [ 'categorical_columns' ]) self . data_profile [ 'numerical_columns' ] = list ( data . select_dtypes ( exclude = [ \"object\" ])) self . data_profile [ 'num_numerical_columns' ] = len ( self . data_profile [ 'numerical_columns' ]) self . data_profile [ 'num_duplicate_rows' ] = data . duplicated () . sum () self . describe = data . describe () . T self . standard_deviation = self . describe [ self . describe [ 'std' ] == 0 ] self . standard_deviation = list ( self . standard_deviation . index ) self . data_profile [ 'num_col_with_std_zero' ] = len ( self . standard_deviation ) if len ( self . standard_deviation ) > 0 : self . data_profile [ 'cols_with_std_zero' ] = self . standard_deviation self . size = data . size / ( 1024 * 1024 ) self . data_profile [ 'datasize' ] = str ( round ( self . size , 2 )) + \" MB\" return self . data_profile","title":"get_data_profile"},{"location":"ForDevelopers/Api/API_preprocess/#separatete-label-features","text":"This is used to separate the target and feature as X and Y def separate_label_feature ( self , data , label_column_name ): self . X = data . drop ( labels = label_column_name , axis = 1 ) # drop the columns specified and separate the feature columns self . Y = data [ label_column_name ] # Filter the Label columns return self . X , self . Y","title":"separatete Label features"},{"location":"ForDevelopers/Api/API_preprocess/#remove_columns","text":"This method removes the given columns from a pandas dataframe. def remove_columns ( self , data , columns ): self . data = data self . columns = columns self . useful_data = self . data . drop ( labels = self . columns , axis = 1 ) # drop the labels specified in the columns return self . useful_data","title":"remove_columns"},{"location":"ForDevelopers/Api/API_preprocess/#impute_missing_values","text":"This method will be used to impute missing values in the dataframe def impute_missing_values ( self , data , mv_flag = None , target = None , strategy = 'mode' , impute_val = None , missing_vals = None ): dataframe = data if mv_flag is True : # Converting missing_vals to Nan Values if missing_vals : dataframe . replace ( missing_vals , np . nan , inplace = True ) # Checking for Missing Values in Dependent Variable if dataframe [ target ] . isna () . any (): dataframe = dataframe [ dataframe [ target ] . notna ()] . copy () # Selecting the Dataframe With No missing values in Dependent column # Checking for Missing Values in Independent Variables Missing_data_columns = dataframe . columns [ dataframe . isna () . any ()] . tolist () # Finding Columns with the missing data from dataframe if strategy == 'fixed' : # checking if strategy == fixed dataframe . fillna ( impute_val , inplace = True ) # Filling the Nan values with the imputed value from user else : for columns in Missing_data_columns : # Iterating over the columns having Nan Values if dataframe [ columns ] . dtype == 'object' : # Checking for the categorical data mode = dataframe [ columns ] . mode ()[ 0 ] dataframe [ columns ] . fillna ( mode , inplace = True ) # Imputing Nan values with mode of the column else : if strategy == 'median' : # checking if the strategy == median median = dataframe [ columns ] . median () dataframe [ columns ] . fillna ( median , inplace = True ) # Imputing Nan values with median of the column else : # The only strategy remains is mean mean = dataframe [ columns ] . mean () dataframe [ columns ] . fillna ( mean , inplace = True ) # Imputing Nan values with mean of the column else : self . logger_object . log ( self . file_object , \"my_flag found False\" ) return dataframe","title":"impute_missing_values"},{"location":"ForDevelopers/Api/API_preprocess/#type_conversion","text":"This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible. def type_conversion ( self , dataset , cat_to_num = None , num_to_cat = None ): if cat_to_num is not None : for column in cat_to_num : dataset [ column ] = pd . to_numeric ( dataset [ column ]) if num_to_cat is not None : for column in num_to_cat : dataset [ column ] = dataset [ column ] . astype ( 'object' ) return dataset","title":"type_conversion"},{"location":"ForDevelopers/Api/API_preprocess/#remove_imbalance","text":"Used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques Input Description: data: the input dataframe with target column. threshold: the threshold of mismatch between the target values to perform balancing. Output: A balanced dataframe. def remove_imbalance ( self , data , target , threshold = 10.0 , oversample = True , smote = False ): X = data . drop ( target , axis = 1 ) y = data [ target ] self . logger_object . log ( self . file_object , 'Class Imbalance Process Starts in the remove_imbalance method of the DataPreprocessor class' ) no_of_classes = data [ target ] . nunique () if no_of_classes == 2 : thresh_satisfied = (( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . any () < threshold ) if thresh_satisfied : if smote : smote = SMOTE () X , y = smote . fit_resample ( X , y ) elif oversample : ROS = RandomOverSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : ROS = RandomUnderSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : high = ( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . ravel () . max () low = ( data [ target ] . value_counts () / float ( len ( data [ target ])) * 100 ) . ravel () . min () thresh_satisfied = ( high - low > 100.0 - threshold ) if thresh_satisfied : if smote : for i in range ( no_of_classes - 2 ): smote = SMOTE () X , y = smote . fit_resample ( X , y ) elif oversample : for i in range ( no_of_classes - 2 ): ROS = RandomOverSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) else : for i in range ( no_of_classes - 2 ): ROS = RandomUnderSampler ( sampling_strategy = 'auto' , random_state = 42 ) X , y = ROS . fit_sample ( X , y ) y . to_frame ( name = target ) dfBalanced = pd . concat ([ X , y ], axis = 1 ) return dfBalanced","title":"remove_imbalance"},{"location":"ForDevelopers/Api/API_preprocess/#remove_columns_with_minimal_variance","text":"This method drops any numerical column with standard deviation below specified threshold Input : data: input DataFrame in which we need to check std deviations : threshold : the threshold for std deviation below which we need to drop the columns Output: A DataFrame with numerical columns with low std dev dropped. def remove_columns_with_minimal_variance ( self , data , threshold ): sel = VarianceThreshold ( threshold = ( threshold * ( 1 - threshold ))) columnlist = list ( data . select_dtypes ( include = 'number' ) . columns ) sel_var = sel . fit_transform ( data [ columnlist ]) new_data = data [ data . columns [ sel . get_support ( indices = True )]] return new_data # return the read data to the calling method","title":"remove_columns_with_minimal_variance"},{"location":"ForDevelopers/Api/API_preprocess/#standardize_data","text":"This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1. * Input : the input dataframe with numeric columns. * Output: Standardized data where mean of each column will be 0 and standard deviation will be 1. def standardize_data ( self , dataframe ): data = dataframe stdscalar = StandardScaler () scaled_data = stdscalar . fit_transform ( data ) scaled_data = pd . Dataframe ( data = scaled_data , columns = data . columns ) return scaled_data","title":"standardize_data"},{"location":"ForDevelopers/Api/API_preprocess/#normalize_data","text":"This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1. * Input : the input dataframe with numeric columns. * Output: Normalized data where minimum value of each column will be 0 and maximum value of each column will be 1. def normalize_data ( self , dataframe ): data = dataframe normalizer = MinMaxScaler () normalized_data = normalizer . fit_transform ( data ) normalized_data = pd . Dataframe ( data = normalized_data , columns = data . columns ) return normalized_data","title":"normalize_data"},{"location":"ForDevelopers/Api/API_preprocess/#pca","text":"This method reduces the dimension from scaled Data which enables quick for large data files. * input : Data which is Scaled, var_explained = 0.90(default value) * Output : It returns the scaled and reduced dimensions. def pca ( self , data , var_explained ): self . data = data self . var_explained = var_explained n = len ( data . keys ()) # find out the no columns in the data mat_pca = PCA ( n_components = n ) mat_pca . fit ( data ) # applying PCA model ##calculate variance ratios variance = mat_pca . explained_variance_ratio_ cum_var = np . cumsum ( np . round ( mat_pca . explained_variance_ratio_ , decimals = 3 ) * 100 )","title":"pca"},{"location":"ForDevelopers/Api/API_preprocess/#calc_num_components","text":"This function is used for calculating number of principal components to use: def calc_num_components ( cum_var , var_explained ): for i in range ( n ): if cum_var [ i ] >= var_explained : return i + 1 # call the function to calulate num_components: n_components = calc_num_components ( cum_var , var_explained ) # create the PCA instance pca = PCA ( n_components = n_components ) principal_components = pca . fit_transform ( data ) # Convert into dataframe pca_data = pd . DataFrame ( data = principal_components , columns = [ 'PC' + str ( i ) for i in range ( 1 , n_components + 1 )]) return pca_data","title":"calc_num_components"},{"location":"ForDevelopers/Api/API_preprocess/#preprocess","text":"This function is used to preprocess the data by calling the apis listed above * input: dataset, target columns, unwanted columns * output: preprocessed target and feature dataset def preprocess ( self , dataset , target_column , unwanted_cols ): dataset = self . remove_columns ( dataset , columns = [ 'Unnamed: 0' ]) dataset = self . impute_missing_values ( data = dataset , mv_flag = True , target = target_column ) # dataset = self.remove_columns(dataset, unwanted_cols) dataset = self . remove_imbalance ( dataset , target_column , threshold = 10.0 , oversample = True , smote = False ) dataset , self . y = self . separate_label_feature ( dataset , target_column ) self . x = self . remove_columns_with_minimal_variance ( data = dataset , threshold = 0.1 ) return self . x , self . y","title":"preprocess"},{"location":"ForDevelopers/Api/API_upload/","text":"DataGetter \u00b6 This class shall be used for obtaining the data from the source for training. Read data from csv \u00b6 def read_data_from_csv ( self , file_name ): self . csv = pd . read_csv ( file_name , sep = ',' , header = 'infer' , names = None , usecols = None ) return self . csv Read data from Excel \u00b6 def read_data_from_excel ( self , file_name ): excel = pd . read_excel ( file_name , sheet_name = 0 , header = 0 , names = None , index_col = None , usecols = None , ) return pd . DataFrame ( excel ) Read data from Json \u00b6 def read_data_from_json ( self , file_name ): json = pd . read_json ( self , file_name ) return pd . DataFrame ( json ) Read data from Html \u00b6 def read_data_from_html ( self , file_name ): html = pd . read_html ( self , file_name ) data = html [ 0 ] return pd . DataFrame ( data ) Connect to SqlDb \u00b6 def Connect_to_sqldb ( self ): connection = sqlalchemy . create_engine ( \"mysql+pymysql://root:*****/*****\" ) return connection Read data from Mongdb \u00b6 def read_data_from_mongdb ( self , file_name ): clinet = pymongo . MongoClinet ( \"mongodb://127.0.0.1:27017/\" ) mydb = client [ \"file_name\" ] return pd . DataFrame ( mydb ) Get Data \u00b6 def get_data ( self , file_type , file ): if file_type == 'CSV' : data = self . read_data_from_csv ( file ) if file_type == 'xlsx' : data = self . read_data_from_excel ( file ) if file_type == 'html' : data = self . read_data_from_html ( file ) if file_type == 'json' : data = self . read_data_from_json ( file ) return data except Exception as e : self . logger . log ( self . log_file_name , 'Exception occured in get_data method of the Data_Getter class. Exception message: ' + str ( e )) raise Exception ()","title":"Data Upload"},{"location":"ForDevelopers/Api/API_upload/#datagetter","text":"This class shall be used for obtaining the data from the source for training.","title":"DataGetter"},{"location":"ForDevelopers/Api/API_upload/#read-data-from-csv","text":"def read_data_from_csv ( self , file_name ): self . csv = pd . read_csv ( file_name , sep = ',' , header = 'infer' , names = None , usecols = None ) return self . csv","title":"Read data from csv"},{"location":"ForDevelopers/Api/API_upload/#read-data-from-excel","text":"def read_data_from_excel ( self , file_name ): excel = pd . read_excel ( file_name , sheet_name = 0 , header = 0 , names = None , index_col = None , usecols = None , ) return pd . DataFrame ( excel )","title":"Read data from Excel"},{"location":"ForDevelopers/Api/API_upload/#read-data-from-json","text":"def read_data_from_json ( self , file_name ): json = pd . read_json ( self , file_name ) return pd . DataFrame ( json )","title":"Read data from Json"},{"location":"ForDevelopers/Api/API_upload/#read-data-from-html","text":"def read_data_from_html ( self , file_name ): html = pd . read_html ( self , file_name ) data = html [ 0 ] return pd . DataFrame ( data )","title":"Read data from Html"},{"location":"ForDevelopers/Api/API_upload/#connect-to-sqldb","text":"def Connect_to_sqldb ( self ): connection = sqlalchemy . create_engine ( \"mysql+pymysql://root:*****/*****\" ) return connection","title":"Connect to SqlDb"},{"location":"ForDevelopers/Api/API_upload/#read-data-from-mongdb","text":"def read_data_from_mongdb ( self , file_name ): clinet = pymongo . MongoClinet ( \"mongodb://127.0.0.1:27017/\" ) mydb = client [ \"file_name\" ] return pd . DataFrame ( mydb )","title":"Read data from Mongdb"},{"location":"ForDevelopers/Api/API_upload/#get-data","text":"def get_data ( self , file_type , file ): if file_type == 'CSV' : data = self . read_data_from_csv ( file ) if file_type == 'xlsx' : data = self . read_data_from_excel ( file ) if file_type == 'html' : data = self . read_data_from_html ( file ) if file_type == 'json' : data = self . read_data_from_json ( file ) return data except Exception as e : self . logger . log ( self . log_file_name , 'Exception occured in get_data method of the Data_Getter class. Exception message: ' + str ( e )) raise Exception ()","title":"Get Data"},{"location":"ForDevelopers/Api/Regression/","text":"Regression models \u00b6 Setup \u00b6 from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import RandomizedSearchCV from sklearn.metrics import r2_score , mean_squared_error import xgboost as xgb from sklearn.neighbors import KNeighborsRegressor import logger By subclassing the Model class: in that case, you should define your layers in init and you should implement the model's forward pass in regressor. class RegressionModelTuner(): \u00b6 This class shall be used to get the best suited Regression model def __init__ ( self ): self . file_object = open ( 'RegressionLogs.txt' , 'a+' ) self . logger_object = logger . App_Logger () Method Name : get_tuned_knn_model \u00b6 def get_tuned_knn_model ( self , x_train , y_train ): Description : This method will be used to get the hypertuned KNN Model x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet output : A hyper parameter tuned model object parameters \u00b6 Let's set up a parameter grid that will be explored during the search. Note that you can use fewer parameters and fewer options for each parameter. Same goes for more parameter and more options if you want to be very thorough. Also, you can plug in any other ML method instead of XGBoost and search for its optimal parameters. knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], } Method Name: get_tuned_random_forest_classifier \u00b6 Description: This method will be used to build RandomForestRegressor model Input Description: x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet Let's try hyperparameter tuning on the all features data This first section is setting up the grid and importing the necessary modules and fitting X_train and y_train self . model = RandomForestRegressor ( n_estimators = n_estimators , max_depth = max_depth , criterion = criterion , min_samples_leaf = min_samples_leaf , max_features = max_features , min_samples_split = min_samples_split , bootstrap = bootstrap , random_state = 25 , n_jobs =- 1 ) self . model = RandomForestRegressor ( n_jobs =- 1 ) self . model . fit ( x_train , y_train ) self . logger_object . log ( self . file_object , Method Name: get_tuned_xgboost_model \u00b6 Description: This method will be used to build XGBoost Regressor model Input Description: x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet Parameters self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] } self . rmdsearch = RandomizedSearchCV ( xgb . XGBRegressor ( objective = 'reg:squarederror' ), param_distributions = self . xg_parameters , n_iter = 10 , cv = 10 , n_jobs =- 1 ) self . rmdsearch . fit ( x_train , y_train ) hyperparameters = self . rmdsearch . best_params_n_estimators , min_child_weight , max_depth , learning_rate , gamma , colsample_bytree = hyperparameters [ 'n_estimators' ], \\ hyperparameters [ 'min_child_weight' ], \\ hyperparameters [ 'max_depth' ], \\ hyperparameters [ 'learning_rate' ], \\ hyperparameters [ 'gamma' ], \\ hyperparameters [ 'colsample_bytree' ] self . xgboost_model = xgb . XGBRegressor ( n_estimators = n_estimators , learning_rate = learning_rate , gamma = gamma , min_child_weight = min_child_weight , max_depth = max_depth , colsample_bytree = colsample_bytree ) Fitting X_train and y_train \u00b6 self . xgboost_model = xgb . XGBRegressor ( objective = 'reg:squarederror' , n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train ) self . logger_object . log ( self . file_object , \"Xgboost Model Training Started.\" )","title":"Regression"},{"location":"ForDevelopers/Api/Regression/#regression-models","text":"","title":"Regression models"},{"location":"ForDevelopers/Api/Regression/#setup","text":"from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import RandomizedSearchCV from sklearn.metrics import r2_score , mean_squared_error import xgboost as xgb from sklearn.neighbors import KNeighborsRegressor import logger By subclassing the Model class: in that case, you should define your layers in init and you should implement the model's forward pass in regressor.","title":"Setup"},{"location":"ForDevelopers/Api/Regression/#class-regressionmodeltuner","text":"This class shall be used to get the best suited Regression model def __init__ ( self ): self . file_object = open ( 'RegressionLogs.txt' , 'a+' ) self . logger_object = logger . App_Logger ()","title":"class RegressionModelTuner():"},{"location":"ForDevelopers/Api/Regression/#method-name-get_tuned_knn_model","text":"def get_tuned_knn_model ( self , x_train , y_train ): Description : This method will be used to get the hypertuned KNN Model x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet output : A hyper parameter tuned model object","title":"Method Name : get_tuned_knn_model"},{"location":"ForDevelopers/Api/Regression/#parameters","text":"Let's set up a parameter grid that will be explored during the search. Note that you can use fewer parameters and fewer options for each parameter. Same goes for more parameter and more options if you want to be very thorough. Also, you can plug in any other ML method instead of XGBoost and search for its optimal parameters. knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], }","title":"parameters"},{"location":"ForDevelopers/Api/Regression/#method-name-get_tuned_random_forest_classifier","text":"Description: This method will be used to build RandomForestRegressor model Input Description: x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet Let's try hyperparameter tuning on the all features data This first section is setting up the grid and importing the necessary modules and fitting X_train and y_train self . model = RandomForestRegressor ( n_estimators = n_estimators , max_depth = max_depth , criterion = criterion , min_samples_leaf = min_samples_leaf , max_features = max_features , min_samples_split = min_samples_split , bootstrap = bootstrap , random_state = 25 , n_jobs =- 1 ) self . model = RandomForestRegressor ( n_jobs =- 1 ) self . model . fit ( x_train , y_train ) self . logger_object . log ( self . file_object ,","title":"Method Name: get_tuned_random_forest_classifier"},{"location":"ForDevelopers/Api/Regression/#method-name-get_tuned_xgboost_model","text":"Description: This method will be used to build XGBoost Regressor model Input Description: x_train : Feature Columns of Training DataSet y_train : Target Column of Training DataSet Parameters self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] } self . rmdsearch = RandomizedSearchCV ( xgb . XGBRegressor ( objective = 'reg:squarederror' ), param_distributions = self . xg_parameters , n_iter = 10 , cv = 10 , n_jobs =- 1 ) self . rmdsearch . fit ( x_train , y_train ) hyperparameters = self . rmdsearch . best_params_n_estimators , min_child_weight , max_depth , learning_rate , gamma , colsample_bytree = hyperparameters [ 'n_estimators' ], \\ hyperparameters [ 'min_child_weight' ], \\ hyperparameters [ 'max_depth' ], \\ hyperparameters [ 'learning_rate' ], \\ hyperparameters [ 'gamma' ], \\ hyperparameters [ 'colsample_bytree' ] self . xgboost_model = xgb . XGBRegressor ( n_estimators = n_estimators , learning_rate = learning_rate , gamma = gamma , min_child_weight = min_child_weight , max_depth = max_depth , colsample_bytree = colsample_bytree )","title":"Method Name: get_tuned_xgboost_model"},{"location":"ForDevelopers/Api/Regression/#fitting-x_train-and-y_train","text":"self . xgboost_model = xgb . XGBRegressor ( objective = 'reg:squarederror' , n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train ) self . logger_object . log ( self . file_object , \"Xgboost Model Training Started.\" )","title":"Fitting X_train and y_train"},{"location":"ForDevelopers/Api/classification/","text":"Classification Model \u00b6 By subclassing the Model class: in that case, you should define your layers in init and you should implement the model's forward pass in classifier. Class ClassificationModelTuner(): \u00b6 def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation () Random_forest_classifier \u00b6 Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. def get_tuned_random_forest_classifier ( self , x_train , y_train ): self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ), } Fitting the Model ( RandomForestClassifier) \u00b6 self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train ) Method Name: get_tuned_xgboost_classifier \u00b6 def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model. Booster Parameters \u00b6 self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] } Fitting the Model [x_train, y_train( xgboost_classifier)] \u00b6 self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train ) Method Name: get_tuned_knn_classifier \u00b6 def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model. Patameters \u00b6 The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 ) Example \u00b6 knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], } Method Name: get_best_model \u00b6 def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object Create best model for Random Forest \u00b6 prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x ) create best model for XGBoost \u00b6 Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x ) Method Name: generate_model_report \u00b6 def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object ## class ClassificationModelTuner(): def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation () Method Name: get_tuned_random_forest_classifier \u00b6 def get_tuned_random_forest_classifier ( self , x_train , y_train ): Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Example \u00b6 self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ), Fitting the Model ( RandomForestClassifier) \u00b6 self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train ) Method Name: get_tuned_xgboost_classifier \u00b6 def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model. Booster Parameters \u00b6 self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] } Fitting the Model [x_train, y_train( xgboost_classifier)] \u00b6 self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train ) Method Name: get_tuned_knn_classifier \u00b6 def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model. Patameters \u00b6 The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 ) Example \u00b6 knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], } Method Name: get_best_model \u00b6 def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object create best model for Random Forest \u00b6 prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x ) Create best model for XGBoost \u00b6 Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x ) Method Name: generate_model_report \u00b6 def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object python Class ClassificationModelTuner(): \u00b6 def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation () Method Name: get_tuned_random_forest_classifier \u00b6 def get_tuned_random_forest_classifier ( self , x_train , y_train ): Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Example \u00b6 self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ), Fitting the Model ( RandomForestClassifier) \u00b6 self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train ) Method Name: get_tuned_xgboost_classifier \u00b6 def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model. Booster Parameters \u00b6 self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] } Fitting the Model [x_train, y_train( xgboost_classifier)] \u00b6 self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train ) Method Name: get_tuned_knn_classifier \u00b6 def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model. Patameters \u00b6 The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 ) Example \u00b6 knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], } Method Name: Get best Model \u00b6 def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object Create best model for Random Forest \u00b6 Prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x ) Create best model for XGBoost \u00b6 Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x ) Method Name: generate model report \u00b6 def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object","title":"Classification"},{"location":"ForDevelopers/Api/classification/#classification-model","text":"By subclassing the Model class: in that case, you should define your layers in init and you should implement the model's forward pass in classifier.","title":"Classification Model"},{"location":"ForDevelopers/Api/classification/#class-classificationmodeltuner","text":"def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation ()","title":"Class ClassificationModelTuner():"},{"location":"ForDevelopers/Api/classification/#random_forest_classifier","text":"Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. def get_tuned_random_forest_classifier ( self , x_train , y_train ): self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ), }","title":"Random_forest_classifier"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-randomforestclassifier","text":"self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train )","title":"Fitting the Model ( RandomForestClassifier)"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_xgboost_classifier","text":"def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model.","title":"Method Name: get_tuned_xgboost_classifier"},{"location":"ForDevelopers/Api/classification/#booster-parameters","text":"self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] }","title":"Booster Parameters"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-x_train-y_train-xgboost_classifier","text":"self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train )","title":"Fitting the Model  [x_train, y_train( xgboost_classifier)]"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_knn_classifier","text":"def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model.","title":"Method Name: get_tuned_knn_classifier"},{"location":"ForDevelopers/Api/classification/#patameters","text":"The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 )","title":"Patameters"},{"location":"ForDevelopers/Api/classification/#example","text":"knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], }","title":"Example"},{"location":"ForDevelopers/Api/classification/#method-name-get_best_model","text":"def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object","title":"Method Name: get_best_model"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-random-forest","text":"prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x )","title":"Create best model for Random Forest"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-xgboost","text":"Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x )","title":"create best model for XGBoost"},{"location":"ForDevelopers/Api/classification/#method-name-generate_model_report","text":"def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object ## class ClassificationModelTuner(): def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation ()","title":"Method Name: generate_model_report"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_random_forest_classifier","text":"def get_tuned_random_forest_classifier ( self , x_train , y_train ): Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.","title":"Method Name: get_tuned_random_forest_classifier"},{"location":"ForDevelopers/Api/classification/#example_1","text":"self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ),","title":"Example"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-randomforestclassifier_1","text":"self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train )","title":"Fitting the Model ( RandomForestClassifier)"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_xgboost_classifier_1","text":"def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model.","title":"Method Name: get_tuned_xgboost_classifier"},{"location":"ForDevelopers/Api/classification/#booster-parameters_1","text":"self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] }","title":"Booster Parameters"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-x_train-y_train-xgboost_classifier_1","text":"self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train )","title":"Fitting the Model  [x_train, y_train( xgboost_classifier)]"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_knn_classifier_1","text":"def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model.","title":"Method Name: get_tuned_knn_classifier"},{"location":"ForDevelopers/Api/classification/#patameters_1","text":"The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 )","title":"Patameters"},{"location":"ForDevelopers/Api/classification/#example_2","text":"knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], }","title":"Example"},{"location":"ForDevelopers/Api/classification/#method-name-get_best_model_1","text":"def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object","title":"Method Name: get_best_model"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-random-forest_1","text":"prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x )","title":"create best model for Random Forest"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-xgboost_1","text":"Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x )","title":"Create best model for XGBoost"},{"location":"ForDevelopers/Api/classification/#method-name-generate_model_report_1","text":"def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object python","title":"Method Name: generate_model_report"},{"location":"ForDevelopers/Api/classification/#class-classificationmodeltuner_1","text":"def __init__ ( self ): self . file_object = open ( 'logs/classificationModelsLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . file_operation = FileOperation ()","title":"Class ClassificationModelTuner():"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_random_forest_classifier_1","text":"def get_tuned_random_forest_classifier ( self , x_train , y_train ): Description: This method will be used to build RandomForestClassifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized RandomForestClassifier model. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.","title":"Method Name: get_tuned_random_forest_classifier"},{"location":"ForDevelopers/Api/classification/#example_3","text":"self . rf_parameters = { 'max_depth' : [ 5 , 10 , 15 , 20 , 25 , None ], 'n_estimators' : range ( 10 , 500 , 50 ), 'criterion' : [ 'gini' , 'entropy' ], 'bootstrap' : [ True , False ], 'min_samples_split' : range ( 2 , 10 , 1 ), 'max_features' : [ 'auto' , 'log2' ], 'min_samples_leaf' : range ( 1 , 10 ),","title":"Example"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-randomforestclassifier_2","text":"self . model = RandomForestClassifier ( n_jobs =- 1 ) self . model . fit ( x_train , y_train )","title":"Fitting the Model ( RandomForestClassifier)"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_xgboost_classifier_2","text":"def get_tuned_xgboost_classifier ( self , x_train , y_train ): Description: This method will be used to build XGBoost Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized XGBoost model.","title":"Method Name: get_tuned_xgboost_classifier"},{"location":"ForDevelopers/Api/classification/#booster-parameters_2","text":"self . xg_parameters = { \"n_estimators\" : [ 10 , 50 , 100 , 200 ], \"learning_rate\" : [ 0.05 , 0.10 , 0.15 , 0.20 , 0.25 , 0.30 ], \"max_depth\" : [ 3 , 4 , 5 , 6 , 8 , 10 , 12 , 15 , 20 ], \"min_child_weight\" : [ 1 , 3 , 5 , 7 ], \"gamma\" : [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ], \"colsample_bytree\" : [ 0.3 , 0.4 , 0.5 , 0.7 ] }","title":"Booster Parameters"},{"location":"ForDevelopers/Api/classification/#fitting-the-model-x_train-y_train-xgboost_classifier_2","text":"self . xgboost_model = XGBClassifier ( n_jobs =- 1 ) self . xgboost_model . fit ( x_train , y_train )","title":"Fitting the Model  [x_train, y_train( xgboost_classifier)]"},{"location":"ForDevelopers/Api/classification/#method-name-get_tuned_knn_classifier_2","text":"def get_tuned_knn_classifier ( self , x_train , y_train ): Description: This method will be used to build KNearestNeighbour Classifier model Input Description: It takes x_train and y_train data for training the model. Output: It return Optimized KNearestNeighbourClassifier model.","title":"Method Name: get_tuned_knn_classifier"},{"location":"ForDevelopers/Api/classification/#patameters_2","text":"The overall parameters have been divided into 3 categories by XGBoost authors: General Parameters: Guide the overall functioning Booster Parameters: Guide the individual booster (tree/regression) at each step Learning Task Parameters: Guide the optimization performed rmdsearch = RandomizedSearchCV ( KNeighborsClassifier (), knn_parameters , n_iter = 10 , cv = 10 , random_state = 22 , n_jobs =- 1 ) rmdsearch . fit ( x_train , y_train ) hyperparameters = rmdsearch . best_params_ n_neighbors , weights , algorithm , leaf_size = hyperparameters [ 'n_neighbors' ], hyperparameters [ 'weights' ], \\ hyperparameters [ 'algorithm' ], hyperparameters [ 'leaf_size' ] model = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , n_jobs =- 1 )","title":"Patameters"},{"location":"ForDevelopers/Api/classification/#example_4","text":"knn_parameters = { 'n_neighbors' : [ 50 , 100 , 200 , 250 , 300 , 350 ], 'weights' : [ 'uniform' , 'distance' ], 'algorithm' : [ 'ball_tree' , 'kd_tree' ], 'leaf_size' : [ 20 , 25 , 30 , 35 , 40 , 45 , 50 ], }","title":"Example"},{"location":"ForDevelopers/Api/classification/#method-name-get-best-model","text":"def get_best_model ( self , x , y ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object","title":"Method Name: Get best Model"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-random-forest_2","text":"Prediction using the Random Forest Algorithm self . random_forest = self . get_tuned_random_forest_classifier ( train_x , train_y ) self . prediction_random_forest = self . random_forest . predict ( test_x )","title":"Create best model for Random Forest"},{"location":"ForDevelopers/Api/classification/#create-best-model-for-xgboost_2","text":"Predictions using the XGBoost Model train_x , test_x , train_y , test_y = train_test_split ( x , y ) self . xgboost = self . get_tuned_xgboost_classifier ( train_x , train_y ) self . prediction_xgboost = self . xgboost . predict ( test_x )","title":"Create best model for XGBoost"},{"location":"ForDevelopers/Api/classification/#method-name-generate-model-report","text":"def generate_model_report ( self , model_object , train_x , train_y , test_x , test_y , num_classes ): Description: Find out the Model which has the best AUC score. Output: The best model name and the model object","title":"Method Name: generate model report"},{"location":"ForDevelopers/Api/file_methods/","text":"Filemethods \u00b6 This class shall be used to save the model after training and load the saved model for prediction. class FileOperation: \u00b6 def __init__ ( self ): self . file_object = open ( 'logs/fileOperationLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . model_directory = 'models/' Save the model file to directory then entered the save_model method of the File_Operation class.create seperate directory for each cluster and remove previously existing models for each clusters then saving the model to file ## Method Name: save_model Description: Save the model file to directory Outcome: File gets saved On Failure: Raise Exception def save_model ( self , model , filename ): Entered the load_model method of the File_Operation class Method Name: find_correct_model_file \u00b6 Description: Select the correct model based on cluster number Output: The Model file On Failure: Raise Exception def find_correct_model_file ( self , cluster_number ):","title":"Data File"},{"location":"ForDevelopers/Api/file_methods/#filemethods","text":"This class shall be used to save the model after training and load the saved model for prediction.","title":"Filemethods"},{"location":"ForDevelopers/Api/file_methods/#class-fileoperation","text":"def __init__ ( self ): self . file_object = open ( 'logs/fileOperationLogs.txt' , 'a+' ) self . logger_object = AppLogger () self . model_directory = 'models/' Save the model file to directory then entered the save_model method of the File_Operation class.create seperate directory for each cluster and remove previously existing models for each clusters then saving the model to file ## Method Name: save_model Description: Save the model file to directory Outcome: File gets saved On Failure: Raise Exception def save_model ( self , model , filename ): Entered the load_model method of the File_Operation class","title":"class FileOperation:"},{"location":"ForDevelopers/Api/file_methods/#method-name-find_correct_model_file","text":"Description: Select the correct model based on cluster number Output: The Model file On Failure: Raise Exception def find_correct_model_file ( self , cluster_number ):","title":"Method Name: find_correct_model_file"},{"location":"ForDevelopers/Api/logger/","text":"Logger \u00b6 When you want to configure logging for your project, you should do it as soon as possible when the program starts. If app.logger is accessed before logging is configured, it will add a default handler. If possible, configure logging before creating the application object. \u00b6 class AppLogger : def __init__ ( self ): self . now = datetime . now () self . date = self . now . date () self . current_time = self . now . strftime ( \"%H:%M:%S\" ) def log ( self , file_object , log_message ): \"\"\"This method will be used for logger all the information to the file.\"\"\" file_object . write ( str ( self . date ) + \"/\" + str ( self . current_time ) + \" \\t\\t \" + log_message + \" \\n \" )","title":"Data Logger"},{"location":"ForDevelopers/Api/logger/#logger","text":"","title":"Logger"},{"location":"ForDevelopers/Api/logger/#when-you-want-to-configure-logging-for-your-project-you-should-do-it-as-soon-as-possible-when-the-program-starts-if-applogger-is-accessed-before-logging-is-configured-it-will-add-a-default-handler-if-possible-configure-logging-before-creating-the-application-object","text":"class AppLogger : def __init__ ( self ): self . now = datetime . now () self . date = self . now . date () self . current_time = self . now . strftime ( \"%H:%M:%S\" ) def log ( self , file_object , log_message ): \"\"\"This method will be used for logger all the information to the file.\"\"\" file_object . write ( str ( self . date ) + \"/\" + str ( self . current_time ) + \" \\t\\t \" + log_message + \" \\n \" )","title":"When you want to configure logging for your project, you should do it as soon as possible when the program starts. If app.logger is accessed before logging is configured, it will add a default handler. If possible, configure logging before creating the application object."},{"location":"ForDevelopers/Api/visualization/","text":"Visualization \u00b6 Setup \u00b6 import scipy from scipy.stats.stats import pearsonr from datetime import datetime import numpy as np import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns This class shall be used to include all Data Visualization techniques to be feed to the Machine Learning Models Method Name: balance_imbalance_check \u00b6 def balance_imbalance_check ( self , dataframe , target ): Description: This method will be used to plot the balance/imbalance datasets using barplot/countplot Input Description: data: the input dataframe with target column. target: target column name. Output: plot of target variable value count. On Failure: Raise Exception x = sns . countplot ( target , data = dataframe ) . set_title ( \"Balance Imbalance Count\" ) sns . barplot ( x = 'is_promoted' , y = 'is_promoted' , data = df , hue = 'is_promoted' , estimator = lambda x : len ( x ) / len ( df ) * 100 ) . set_title ( \"Balance Imbalance Count\" ) Method Name: corelation_heatmap \u00b6 def corelation_heatmap ( self , dataframe ): Description: This method will be used to plot the heatmap to show the corelation among the variables Input Description: data: the input dataframe with target column. Output: plot of heatmap that shows the corelation among the variable. On Failure: Raise Exception data = dataframe . select_dtypes ( include = [ np . number ]) plt . figure ( figsize = ( 20 , 10 )) sns . set_palette ( \"PuBuGn_d\" ) plot = sns . heatmap ( data . corr (), annot = True , cmap = 'RdYlGn' ) plot . figure . savefig ( \"static/graphs/correlation.png\" )","title":"Data Visualization"},{"location":"ForDevelopers/Api/visualization/#visualization","text":"","title":"Visualization"},{"location":"ForDevelopers/Api/visualization/#setup","text":"import scipy from scipy.stats.stats import pearsonr from datetime import datetime import numpy as np import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns This class shall be used to include all Data Visualization techniques to be feed to the Machine Learning Models","title":"Setup"},{"location":"ForDevelopers/Api/visualization/#method-name-balance_imbalance_check","text":"def balance_imbalance_check ( self , dataframe , target ): Description: This method will be used to plot the balance/imbalance datasets using barplot/countplot Input Description: data: the input dataframe with target column. target: target column name. Output: plot of target variable value count. On Failure: Raise Exception x = sns . countplot ( target , data = dataframe ) . set_title ( \"Balance Imbalance Count\" ) sns . barplot ( x = 'is_promoted' , y = 'is_promoted' , data = df , hue = 'is_promoted' , estimator = lambda x : len ( x ) / len ( df ) * 100 ) . set_title ( \"Balance Imbalance Count\" )","title":"Method Name: balance_imbalance_check"},{"location":"ForDevelopers/Api/visualization/#method-name-corelation_heatmap","text":"def corelation_heatmap ( self , dataframe ): Description: This method will be used to plot the heatmap to show the corelation among the variables Input Description: data: the input dataframe with target column. Output: plot of heatmap that shows the corelation among the variable. On Failure: Raise Exception data = dataframe . select_dtypes ( include = [ np . number ]) plt . figure ( figsize = ( 20 , 10 )) sns . set_palette ( \"PuBuGn_d\" ) plot = sns . heatmap ( data . corr (), annot = True , cmap = 'RdYlGn' ) plot . figure . savefig ( \"static/graphs/correlation.png\" )","title":"Method Name: corelation_heatmap"},{"location":"GettingStarted/GettingStarted/","text":"Getting Started \u00b6 Welcome to AutoNeuro \u00b6 AutoNeuro is an automated machine learning application built using python 3.7. It allows users to build production ready ML models with ease and efficiency. It is an end to end automated machine learning solution where the user will only give dataset in recognizable formats and select the type of the problem and the result will be the best performing hyper tuned machine learning model. The descriptive and graphical analysis of the data is also displayed. The user will also get privileges to choose the deployment option. (eg: AWS, GCP, etc). Features \u00b6 \u2022 Data sets automatically analyzed \u2022 Machine learning Modelling techniques using both classification and Regression \u2022 Evaluations of the models for easy comparison and reuse \u2022 Automated model building based on the type of problem. \u2022 Output model is hyper-parameter tuned model \u2022 Choice based automated deployment to various clouds. \u2022 Provision of user friendly interface to select input and target features","title":"Introduction & Features"},{"location":"GettingStarted/GettingStarted/#getting-started","text":"","title":"Getting Started"},{"location":"GettingStarted/GettingStarted/#welcome-to-autoneuro","text":"AutoNeuro is an automated machine learning application built using python 3.7. It allows users to build production ready ML models with ease and efficiency. It is an end to end automated machine learning solution where the user will only give dataset in recognizable formats and select the type of the problem and the result will be the best performing hyper tuned machine learning model. The descriptive and graphical analysis of the data is also displayed. The user will also get privileges to choose the deployment option. (eg: AWS, GCP, etc).","title":"Welcome to AutoNeuro"},{"location":"GettingStarted/GettingStarted/#features","text":"\u2022 Data sets automatically analyzed \u2022 Machine learning Modelling techniques using both classification and Regression \u2022 Evaluations of the models for easy comparison and reuse \u2022 Automated model building based on the type of problem. \u2022 Output model is hyper-parameter tuned model \u2022 Choice based automated deployment to various clouds. \u2022 Provision of user friendly interface to select input and target features","title":"Features"},{"location":"GettingStarted/How%20to%20use/","text":"Let\u2019s get started: \u00b6 In this section we will install AutoNeuro in our system.We can install AutoNeuro through many ways. Hardware Requirements \u00b6 Model training \u00b6 - 8 GB RAM - 4 GB of Hard Disk Space - Intel Core i5 Processor(recommended) Model testing \u00b6 - 4 GB RAM - 2 GB of Hard Disk Space - Intel Core i5 Processor(recommended) Installation \u00b6 Open Command Prompt/terminal and type pip install AutoNeuro. This will install latest stable version of AutoNeuro in your system. # Requires the latest pip pip install -- upgrade pip # Current stable release for CPU and GPU pip install autoneuro","title":"Installation"},{"location":"GettingStarted/How%20to%20use/#lets-get-started","text":"In this section we will install AutoNeuro in our system.We can install AutoNeuro through many ways.","title":"Let\u2019s get started:"},{"location":"GettingStarted/How%20to%20use/#hardware-requirements","text":"","title":"Hardware Requirements"},{"location":"GettingStarted/How%20to%20use/#model-training","text":"- 8 GB RAM - 4 GB of Hard Disk Space - Intel Core i5 Processor(recommended)","title":"Model training"},{"location":"GettingStarted/How%20to%20use/#model-testing","text":"- 4 GB RAM - 2 GB of Hard Disk Space - Intel Core i5 Processor(recommended)","title":"Model testing"},{"location":"GettingStarted/How%20to%20use/#installation","text":"Open Command Prompt/terminal and type pip install AutoNeuro. This will install latest stable version of AutoNeuro in your system. # Requires the latest pip pip install -- upgrade pip # Current stable release for CPU and GPU pip install autoneuro","title":"Installation"},{"location":"User%27s%20Guide/Clouddeployment/","text":"Deploying Model to Cloud \u00b6 Once you are satisfied with the model Select the cloud Service in which you want to deploy model. We offer deployment in Azure, GCP, AWS. Click on Deploy to proceed further.","title":"Cloud deployment"},{"location":"User%27s%20Guide/Clouddeployment/#deploying-model-to-cloud","text":"Once you are satisfied with the model Select the cloud Service in which you want to deploy model. We offer deployment in Azure, GCP, AWS. Click on Deploy to proceed further.","title":"Deploying Model to Cloud"},{"location":"User%27s%20Guide/Data%20Visualization/","text":"Data Visualization \u00b6","title":"Data Visualization"},{"location":"User%27s%20Guide/Data%20Visualization/#data-visualization","text":"","title":"Data Visualization"},{"location":"User%27s%20Guide/LoggingHelp/","text":"Logging Help \u00b6","title":"Logging Help"},{"location":"User%27s%20Guide/LoggingHelp/#logging-help","text":"","title":"Logging Help"},{"location":"User%27s%20Guide/ModelTraining/","text":"Model Training \u00b6 On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data. Training the data \u00b6 After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Model Training"},{"location":"User%27s%20Guide/ModelTraining/#model-training","text":"On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data.","title":"Model Training"},{"location":"User%27s%20Guide/ModelTraining/#training-the-data","text":"After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Training the data"},{"location":"User%27s%20Guide/Predictions/","text":"Predictions \u00b6 Once you have successfully trained the data you will be able to make predictions. Press \u2018Try Prediction\u2019 button at the bottom of the report for making Predictions.","title":"Predictions"},{"location":"User%27s%20Guide/Predictions/#predictions","text":"Once you have successfully trained the data you will be able to make predictions. Press \u2018Try Prediction\u2019 button at the bottom of the report for making Predictions.","title":"Predictions"},{"location":"User%27s%20Guide/Troubleshooting/","text":"Troubleshooting \u00b6","title":"Troubleshooting"},{"location":"User%27s%20Guide/Troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"User%27s%20Guide/Uploading%20Data/","text":"Uploading Data \u00b6 The process for Uploading Data Are as follows: Getting HomeScreen for AutoNeuro \u00b6 Note Here we will explain what code user has to write to start the process. For now I have written steps for running app.py If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen. Adding Data Set for Training \u00b6 To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page.","title":"Uploading Data"},{"location":"User%27s%20Guide/Uploading%20Data/#uploading-data","text":"The process for Uploading Data Are as follows:","title":"Uploading Data"},{"location":"User%27s%20Guide/Uploading%20Data/#getting-homescreen-for-autoneuro","text":"Note Here we will explain what code user has to write to start the process. For now I have written steps for running app.py If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen.","title":"Getting HomeScreen for AutoNeuro"},{"location":"User%27s%20Guide/Uploading%20Data/#adding-data-set-for-training","text":"To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page.","title":"Adding Data Set for Training"}]}