{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"Applicationworkflow/","text":"Workflow Data Ingestion and File Conversion Data Ingestion: Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse, data mart, database, or a document store. Data Connector Utils File Conversion Utils Microsoft Access CSV & text files, PDF Spatial File JSON Statistical File HTML Tableau Server or Tableau Online Excel files Actian Matrix openDocument Spreadsheets Actian Vectorwise Binary Excel (.xlsb) files Alibaba AnalyticDB for MySQL Clipboard Alibaba Data Lake Analytics Pickling Alibaba MaxCompute msgpack Amazon Athena HDF5 (PyTables) Amazon Aurora for MySQL Feather Amazon EMR Hadoop Hive Parquet Amazon Redshift ORC Anaplan Google BigQuery Apache Drill Stata format Aster Database SAS formats Azure SQL Synapse Analytics SPSS formats Box Other file formats Cloudera Hadoop Performance considerations Databricks Denodo Dropbo Esri ArcGIS Server Exasol Firebird 3 Google Ads Google Analytics Google BigQuery Google Cloud SQL Google Drive Google Sheets Hortonworks Hadoop Hive IBM BigInsights IBM DB2 IBM PDA (Netezza) Impala Intuit QuickBooks Online Kognitio Kyvos LinkedIn Sales Navigator MapR Hadoop Hive MariaDB arketo MarkLogic MemSQL Microsoft Analysis Services Microsoft PowerPivot Microsoft SQL Server MonetDB MongoDB BI Connector MySQL OData OneDrive Oracle Oracle Eloqua Oracle Essbase Pivotal Greenplum PostgreSQL Presto Progress OpenEdge Qubole Presto Salesforce Splunk SAP HANA SAP NetWeaver Business Warehouse SAP Sybase ASE SAP Sybase IQ ServiceNow ITSM SharePoint Lists Snowflake Spark SQL Connector Plugin Web Data Connector Other Databases (JDBC) Other Databases (ODBC) Phase 2: Data Connector Utils File Conversion Utils Spatial File OpenDocument Spreadsheets Statistical File Tableau Server or Tableau Online Actian Matrix Teradata OLAP Connector TIBCO Data Virtualization Vertica Teradata Class Name DataGetter Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name read_data_from_json Method Description This method will be used to read data from a json file. Input parameter names self,file_name Input Parameter Description file_name: name of the file to be read ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name read_data_from_html Method Description This method will be used to read data from an HTML web page Input parameter names self,url Input Parameter Description url: URL of the HTML page to be read. ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name read_data_from_excel Method Description This method will be used to read data from an MS Excel File Input parameter names self,file_name,sheet_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read sheet_name: Lists of strings/integers are used to request multiple sheets. Specify None to get all sheets. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name Connect_to_sqldb Method Description This method will be used to connect to a SQL Databases Input parameter names self,host,port, username, password Input Parameter Description host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server ouptput A DB connection object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name read_data_from_sqldb Method Description This method will be used to read data from SQL Databases Input parameter names self,db_name,host,port, username, password, schema_name,query_string Input Parameter Description db_name: For example, SQL, MySQL, SQLLite etc. host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server schema_name: The name of the DB schema the user wants to connect to. query_string: the query to be executed to load the data ouptput A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Method Name read_data_from_mongdb Method Description This method will be used to read data from Mongo DB Input parameter names self,host,port, username, password, db_name,collection_name, query_string. Input Parameter Description output A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message Data Profiling Data profiling is the process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects. After reading the data, automatically the following details should be shown: a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM Method Definition Class Name DataProfiler Method Name get_data_profile Method Description This method will be used to give various insighst about data. Input parameter names self, dataframe Input Parameter Description dataframe: the inpt data just loaded from source ouptput a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM On Exception Write the exception in the log file. Raise an exception with the appropriate error message Stats based EDA: MVP : Minimum Viable Product Building a minimum viable product is an opportunity to quickly test an idea and see if it works. It doesn't need to be the best solution, it should just be a usable solution. If it is, then we can move on to the development stage and make it work well. OLS : Ordinary least squares The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values. VIF : The variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. Correlation: Correlation is a statistical technique that can show whether and how strongly pairs of variables are related Phase1: Anova test: An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis. Basically, you're testing groups to see if there's a difference between them. Chi-square test Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. Z-test A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test. T test The t test is one type of inferential statistics. It is used to determine whether there is a significant difference between the means of two groups. With all inferential statistics, we assume the dependent variable fits a normal distribution Weight of evidence The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. F-test An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Technical solution design Exceptions Scenarios Module Wise Step Exception Mitigation Column has mixed values(Integer & number) Give proper error message Ask the user to correct the data. Not all values are numbers Handle Internally Convert categorical to numerical values Phase 2: Seasonality: Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal. Stationary Data: A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time 4 Graph-Based EDA Create the following graphs: MVP: Correlation Heatmaps Check for balance/imbalance Phase1: Count plots Boxplot for outliers Piecharts for categories Geographical plots for scenarios Line charts for trends Barplots Area Charts KDE Plots Stacked charts Scatterplot Phase 2: Word maps PACF ACF Add Custom controls sliders etc Graphical EDA: Class Name DataVisualization Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation Wrong input to the methods Handle Internally Code should never give a wrong input Phase 2: A time series can have components like trend, seasonality, cyclic and residual. ACF considers all these components while finding correlations hence it's a 'complete auto-correlation plot'. PACF is a partial auto-correlation function. More about sliders: https://plotly.com/python/sliders/ 5. Library Based Utils Technical solution design Exceptions Scenarios Module Wise 6. Data Transformers( Pre-processing steps) MVP: Null value handling Categorical to numerical Imbalanced data set handling Handling columns with std deviation zero or below a threshold Normalisation PCA Phase1: Outlier detection Data Scaling/ Normalisation Feature Selection: https://scikit-learn.org/stable/auto_examples/index.html#feature-selection Class Name DataPreprocessor Method Name impute_missing_values Method Description This method will be used to read data from a csv file or a flat file. Input parameter names self,file_name, header,names, use_cols, separator. Input Parameter Description file_name: name of the file to be read. header: Row number(s) to be used as column names. names : array-like, optional. List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns. Separator: Delimiter to use. output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input 7 ML Model Selection: Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. Model selection is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.) and across models of the same type configured with different model hyperparameters (e.g. different kernels in an SVM). MVP: 3 Models\u2014KNN, RandomForest, XGBoost Phase1: Model Selection criteria Technical solution design Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input 8. Model Tuning and Optimization** Note: The data should have been divided into train and validation set before this. Methods for hyper tuning all kinds of models. Regression: Linear Regression Decision Tree Random Forest XG Boost Support Vector Regressor KNN Regressor Model selection criteria: MSE, RMSE, R squared, adjusted R squared Classification: Logistic Regression Decision Tree Random Forest XG Boost Support Vector Classifier KNN Classifier Na\u00efve Baye\u2019s Model selection criteria: Accuracy, AUC, Precision, Recall, F Beta Clustering: K-Means Hierarchial DBSCAN Phase 2: GLM GAM ( https://www.statsmodels.org/stable/regression.html ) Time Series Anomaly Detection Novelty Detection Optics Gaussian Mixtures BIRCH NLP Deep Learning Regularization modules if necessary Class Name ModelTuner Method Name get_tuned_knn_model Method Description This method will be used to get the hypertuned KNN Model Input parameter names self,data Input Parameter Description Data: the training data Hyperparameters to tune ouptput A hyper parameter tuned model object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation 9. Testing Modules Divide the training data itself into train and test sets Use test data to have tests run on the three best models Give the test report a) R2 Score b) Adjusted R2 score c) MSE d) Accuracy e) Precision f) Recall g) F Beta h) Cluster Purity i) Silhouette score Phase 2 AIC BIC Note: Save the best model after validation is completed. Step Exception Mitigation Number of Parameters do not match Handle internally Check the test data creation and verify the columns Only once class present in test data Handle Internally 10. Prediction Pipeline Use the existing data read modules Use the existing pre-processing module Load the model into memory Do predictions Store prediction results(show sample predictions) Phase 2: UI for predictions Step Exception Mitigation Columns don\u2019t match in training and Prediction data Show error message The user enters the correct data 11 Deployment Strategy Take the cloud name as input Prepare the metadata files based on cloud Phase 2: Accept the user credentials Prepare a script file to push changes Docker instance Push of the docker instance to cloud Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging 12. Monitoring Phase 2 No. Of predictions for individual classes No. of predictions (per day, per hour, per week etc.) No. of hits Training data size (number of rows) Time spent in training Failures 13 Logging Separate Folder for logs Logging of every step Entry to the methods Exit from the methods with success/ failure message Error message Logging Model comparisons Training start and end Prediction start and end Achieve asynchronous logging Phase 2: Options for Logging in DB Options for Log Publish Class Name App Logger Method Name log Method Description This method will be used for logging all the information to the file. Input parameter names self,file_object, log_message Input Parameter Description file_object: the file where the logs will be written log_message: the message to be logged ouptput A log file with messages from datetime import datetime class App_Logger : def __init__ ( self ): pass def log ( self , file_object , log_message ): \u201c\u201d\u201d This method will be used for logging all the information to the file . \u201d\u201d\u201d self . now = datetime . now () self . date = self . now . date () self . current_time = self . now . strftime ( \"%H:%M:%S\" ) file_object . write ( str ( self . date ) + \"/\" + str ( self . current_time ) + \" \\t\\t \" + log_message + \" \\n \" ) 13.3 Exceptions Scenarios Module Wise \\ \\ \\ \\ \\ \\ \\ \\ \\","title":"Applicationworkflow"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/","text":"Contributing to autoneuro \u00b6","title":"Help us to improve"},{"location":"Contributingtoautoneuro/Contributingtoautoneuro/#contributing-to-autoneuro","text":"","title":"Contributing to autoneuro"},{"location":"ForDevelopers/APIs/","text":"APIS \u00b6","title":"List of APIs"},{"location":"ForDevelopers/APIs/#apis","text":"","title":"APIS"},{"location":"ForDevelopers/Applicationworkflow/","text":"Application Workflow \u00b6 Exception Scenarios \u00b6 Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message User gives wrong null symbol Give proper error message Ask the user to provide correct symbol used for missing values If the cluster contains only one class No error message required Handle this exception internally. User doesn\u2019t know. Deployment credentials are wrong Give proper error message Ask for the details to be entered again","title":"Workflow"},{"location":"ForDevelopers/Applicationworkflow/#application-workflow","text":"","title":"Application Workflow"},{"location":"ForDevelopers/Applicationworkflow/#exception-scenarios","text":"Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message User gives wrong null symbol Give proper error message Ask the user to provide correct symbol used for missing values If the cluster contains only one class No error message required Handle this exception internally. User doesn\u2019t know. Deployment credentials are wrong Give proper error message Ask for the details to be entered again","title":"Exception Scenarios"},{"location":"ForDevelopers/MethodsforModelbuilding/","text":"Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. Model selection is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.) and across models of the same type configured with different model hyperparameters (e.g. different kernels in an SVM). Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input 8. Model Tuning and Optimization** Divide into train and test \u00b6 The data should have been divided into train and validation set before this. Methods for hyper tuning all kinds of models. Regression: * Linear Regression * Polinomial Regression Model selection criteria: 1. MSE 1. RMSE 1. R squared 1. adjusted R squared Classification: * Logistic Regression * Decision Tree * Random Forest * XG Boost * Support Vector Classifier * KNN Classifier * Na\u00efve Baye\u2019s Model selection criteria: 1. Accuracy 1. AUC 1. Precision 1. Recall 1. F Beta Clustering: * K-Means * Hierarchial * DBSCAN Class Name ModelTuner Method Name get_tuned_knn_model Method Description This method will be used to get the hypertuned KNN Model Input parameter names self,data Input Parameter Description Data: the training data Hyperparameters to tune ouptput A hyper parameter tuned model object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation","title":"Model building"},{"location":"ForDevelopers/MethodsforModelbuilding/#divide-into-train-and-test","text":"The data should have been divided into train and validation set before this. Methods for hyper tuning all kinds of models. Regression: * Linear Regression * Polinomial Regression Model selection criteria: 1. MSE 1. RMSE 1. R squared 1. adjusted R squared Classification: * Logistic Regression * Decision Tree * Random Forest * XG Boost * Support Vector Classifier * KNN Classifier * Na\u00efve Baye\u2019s Model selection criteria: 1. Accuracy 1. AUC 1. Precision 1. Recall 1. F Beta Clustering: * K-Means * Hierarchial * DBSCAN Class Name ModelTuner Method Name get_tuned_knn_model Method Description This method will be used to get the hypertuned KNN Model Input parameter names self,data Input Parameter Description Data: the training data Hyperparameters to tune ouptput A hyper parameter tuned model object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise Step Exception Mitigation","title":"Divide into train and test"},{"location":"ForDevelopers/MethodsforMonitoring/","text":"","title":"MethodsforMonitoring"},{"location":"ForDevelopers/Methodsfordataprofiler/","text":"Data profiling \u00b6 Data profiling is the process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects. Method Definition Data Profiler \u00b6 Class Name DataProfiler Method Name get_data_profile Method Description This method will be used to give various insighst about data. Input parameter names self, dataframe Input Parameter Description dataframe: the inpt data just loaded from source ouptput a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data profiling"},{"location":"ForDevelopers/Methodsfordataprofiler/#data-profiling","text":"Data profiling is the process of reviewing source data, understanding structure, content and interrelationships, and identifying potential for data projects. Method Definition","title":"Data profiling"},{"location":"ForDevelopers/Methodsfordataprofiler/#data-profiler","text":"Class Name DataProfiler Method Name get_data_profile Method Description This method will be used to give various insighst about data. Input parameter names self, dataframe Input Parameter Description dataframe: the inpt data just loaded from source ouptput a) The number of rows b) The number of columns c) Number of missing values per column and their percentage d) Total missing values and it\u2019s percentage e) Number of categorical columns and their list f) Number of numerical columns and their list g) Number of duplicate rows h) Number of columns with zero standard deviation and their list i) Size occupied in RAM On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Profiler"},{"location":"ForDevelopers/Methodsfordataupload/","text":"Data Ingestion and File Conversion \u00b6 Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse, data mart, database, or a document store. Data Connector Utils File Conversion Utils Microsoft Access CSV & text files, PDF Spatial File JSON Statistical File HTML Tableau Server or Tableau Online Excel files Actian Matrix openDocument Spreadsheets Actian Vectorwise Binary Excel (.xlsb) files Alibaba AnalyticDB for MySQL Clipboard Alibaba Data Lake Analytics Pickling Alibaba MaxCompute msgpack Amazon Athena HDF5 (PyTables) Amazon Aurora for MySQL Feather Amazon EMR Hadoop Hive Parquet Amazon Redshift ORC Anaplan Google BigQuery Apache Drill Stata format Aster Database SAS formats Azure SQL Synapse Analytics SPSS formats Box Other file formats Cloudera Hadoop Performance considerations Databricks Denodo Dropbo Esri ArcGIS Server Exasol Firebird 3 Google Ads Google Analytics Google BigQuery Google Cloud SQL Google Drive Google Sheets Hortonworks Hadoop Hive IBM BigInsights IBM DB2 IBM PDA (Netezza) Impala Intuit QuickBooks Online Kognitio Kyvos LinkedIn Sales Navigator MapR Hadoop Hive MariaDB arketo MarkLogic MemSQL Microsoft Analysis Services Microsoft PowerPivot Microsoft SQL Server MonetDB MongoDB BI Connector MySQL OData OneDrive Oracle Oracle Eloqua Oracle Essbase Pivotal Greenplum PostgreSQL Presto Progress OpenEdge Qubole Presto Salesforce Splunk SAP HANA SAP NetWeaver Business Warehouse SAP Sybase ASE SAP Sybase IQ ServiceNow ITSM SharePoint Lists Snowflake Spark SQL Connector Plugin Web Data Connector Other Databases (JDBC) Other Databases (ODBC) Upload Data \u00b6 Read from csv \u00b6 Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from json \u00b6 Method Name read_data_from_json Method Description This method will be used to read data from a json file. Input parameter names self,file_name Input Parameter Description file_name: name of the file to be read ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from html \u00b6 Method Name read_data_from_html Method Description This method will be used to read data from an HTML web page Input parameter names self,url Input Parameter Description url: URL of the HTML page to be read. ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from Excel \u00b6 Method Name read_data_from_excel Method Description This method will be used to read data from an MS Excel File Input parameter names self,file_name,sheet_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read sheet_name: Lists of strings/integers are used to request multiple sheets. Specify None to get all sheets. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Connecting to sqldb \u00b6 Method Name Connect_to_sqldb Method Description This method will be used to connect to a SQL Databases Input parameter names self,host,port, username, password Input Parameter Description host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server ouptput A DB connection object On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from sqldb \u00b6 Method Name read_data_from_sqldb Method Description This method will be used to read data from SQL Databases Input parameter names self,db_name,host,port, username, password, schema_name,query_string Input Parameter Description db_name: For example, SQL, MySQL, SQLLite etc. host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server schema_name: The name of the DB schema the user wants to connect to. query_string: the query to be executed to load the data ouptput A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Read from mongodb \u00b6 Method Name read_data_from_mongdb Method Description This method will be used to read data from Mongo DB Input parameter names self,host,port, username, password, db_name,collection_name, query_string. Input Parameter Description output A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios \u00b6 Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message","title":"Data upload"},{"location":"ForDevelopers/Methodsfordataupload/#data-ingestion-and-file-conversion","text":"Data ingestion is the transportation of data from assorted sources to a storage medium where it can be accessed, used, and analyzed by an organization. The destination is typically a data warehouse, data mart, database, or a document store. Data Connector Utils File Conversion Utils Microsoft Access CSV & text files, PDF Spatial File JSON Statistical File HTML Tableau Server or Tableau Online Excel files Actian Matrix openDocument Spreadsheets Actian Vectorwise Binary Excel (.xlsb) files Alibaba AnalyticDB for MySQL Clipboard Alibaba Data Lake Analytics Pickling Alibaba MaxCompute msgpack Amazon Athena HDF5 (PyTables) Amazon Aurora for MySQL Feather Amazon EMR Hadoop Hive Parquet Amazon Redshift ORC Anaplan Google BigQuery Apache Drill Stata format Aster Database SAS formats Azure SQL Synapse Analytics SPSS formats Box Other file formats Cloudera Hadoop Performance considerations Databricks Denodo Dropbo Esri ArcGIS Server Exasol Firebird 3 Google Ads Google Analytics Google BigQuery Google Cloud SQL Google Drive Google Sheets Hortonworks Hadoop Hive IBM BigInsights IBM DB2 IBM PDA (Netezza) Impala Intuit QuickBooks Online Kognitio Kyvos LinkedIn Sales Navigator MapR Hadoop Hive MariaDB arketo MarkLogic MemSQL Microsoft Analysis Services Microsoft PowerPivot Microsoft SQL Server MonetDB MongoDB BI Connector MySQL OData OneDrive Oracle Oracle Eloqua Oracle Essbase Pivotal Greenplum PostgreSQL Presto Progress OpenEdge Qubole Presto Salesforce Splunk SAP HANA SAP NetWeaver Business Warehouse SAP Sybase ASE SAP Sybase IQ ServiceNow ITSM SharePoint Lists Snowflake Spark SQL Connector Plugin Web Data Connector Other Databases (JDBC) Other Databases (ODBC)","title":"Data Ingestion and File Conversion"},{"location":"ForDevelopers/Methodsfordataupload/#upload-data","text":"","title":"Upload Data"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-csv","text":"Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from csv"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-json","text":"Method Name read_data_from_json Method Description This method will be used to read data from a json file. Input parameter names self,file_name Input Parameter Description file_name: name of the file to be read ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from json"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-html","text":"Method Name read_data_from_html Method Description This method will be used to read data from an HTML web page Input parameter names self,url Input Parameter Description url: URL of the HTML page to be read. ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from html"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-excel","text":"Method Name read_data_from_excel Method Description This method will be used to read data from an MS Excel File Input parameter names self,file_name,sheet_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read sheet_name: Lists of strings/integers are used to request multiple sheets. Specify None to get all sheets. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use ouptput A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from Excel"},{"location":"ForDevelopers/Methodsfordataupload/#connecting-to-sqldb","text":"Method Name Connect_to_sqldb Method Description This method will be used to connect to a SQL Databases Input parameter names self,host,port, username, password Input Parameter Description host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server ouptput A DB connection object On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Connecting to sqldb"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-sqldb","text":"Method Name read_data_from_sqldb Method Description This method will be used to read data from SQL Databases Input parameter names self,db_name,host,port, username, password, schema_name,query_string Input Parameter Description db_name: For example, SQL, MySQL, SQLLite etc. host: the server hostname/IP where the DB server is hosted Port: the port at which the DB Server is running username: The username to connect to the DB server password: The password to connect to the DB server schema_name: The name of the DB schema the user wants to connect to. query_string: the query to be executed to load the data ouptput A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from sqldb"},{"location":"ForDevelopers/Methodsfordataupload/#read-from-mongodb","text":"Method Name read_data_from_mongdb Method Description This method will be used to read data from Mongo DB Input parameter names self,host,port, username, password, db_name,collection_name, query_string. Input Parameter Description output A Pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Read from mongodb"},{"location":"ForDevelopers/Methodsfordataupload/#exceptions-scenarios","text":"Step Exception Mitigation User gives Wrong Data Source Give proper error message Ask the user to re-enter the details User gives corrupted data Give proper error message","title":"Exceptions Scenarios"},{"location":"ForDevelopers/Methodsfordeployment/","text":"Deployment Strategy Take the cloud name as input Prepare the metadata files based on cloud Step Exception Mitigation Wrong Cloud credentials Show error message The user enters the correct data Docker instance not working Show error message Fix the error Cloud push failed Show the error Make corrections to the metadata files Cloud app not starting Ask the user for cloud logs for debugging","title":"Deployment"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/","text":"Once the pandas dataframe is given, using data visualization methods we will get corresponding data in the form of graph. Correlation Heatmaps \u00b6 Check for balance/imbalance \u00b6 Data Visualisation \u00b6 Class Name DataVisualization Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise \u00b6 Step Exception Mitigation Wrong input to the methods Handle Internally Code should never give a wrong input","title":"Data Visualization(Graphical Analysis)"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#correlation-heatmaps","text":"","title":"Correlation Heatmaps"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#check-for-balanceimbalance","text":"","title":"Check for balance/imbalance"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#data-visualisation","text":"Class Name DataVisualization Method Name read_data_from_csv Method Description This method will be used to read data from a csv file or a flat file Input parameter names self,file_name, header,names, use_cols, separator Input Parameter Description file_name: name of the file to be read header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Visualisation"},{"location":"ForDevelopers/MethodsforgraphbasedEDA/#exceptions-scenarios-module-wise","text":"Step Exception Mitigation Wrong input to the methods Handle Internally Code should never give a wrong input","title":"Exceptions Scenarios Module Wise"},{"location":"ForDevelopers/Methodsforlogging/","text":"Separate Folder for logs Logging of every step Entry to the methods Exit from the methods with success/ failure message Error message Logging Model comparisons Training start and end Prediction start and end Achieve asynchronous logging Class Name App Logger Method Name log Method Description This method will be used for logging all the information to the file. Input parameter names self,file_object, log_message Input Parameter Description file_object: the file where the logs will be written log_message: the message to be logged ouptput A log file with messages from datetime import datetime class App_Logger : def __init__ ( self ): pass def log ( self , file_object , log_message ): \u201c\u201d\u201d This method will be used for logging all the information to the file . \u201d\u201d\u201d self . now = datetime . now () self . date = self . now . date () self . current_time = self . now . strftime ( \"%H:%M:%S\" ) file_object . write ( str ( self . date ) + \"/\" + str ( self . current_time ) + \" \\t\\t \" + log_message + \" \\n \" ) 13.3 Exceptions Scenarios Module Wise \\ \\ \\ \\ \\ \\ \\ \\ \\","title":"Logging"},{"location":"ForDevelopers/Methodsforprediction/","text":"Testing Modules \u00b6 About It Divide the training data itself into train and test sets Use test data to have tests run on the three best models Give the test report : R2 Score Adjusted R2 score MSE Accuracy Precision Recall F Beta Cluster Purity Silhouette score Done Step Exception Mitigation Number of Parameters do not match Handle internally Check the test data creation and verify the columns Only once class present in test data Handle Internally _","title":"Model Testing & Selection"},{"location":"ForDevelopers/Methodsforprediction/#testing-modules","text":"About It Divide the training data itself into train and test sets Use test data to have tests run on the three best models Give the test report : R2 Score Adjusted R2 score MSE Accuracy Precision Recall F Beta Cluster Purity Silhouette score Done Step Exception Mitigation Number of Parameters do not match Handle internally Check the test data creation and verify the columns Only once class present in test data Handle Internally _","title":"Testing Modules"},{"location":"ForDevelopers/Methodsforpreprocessing/","text":"Transformations \u00b6 Imputing missing values \u00b6 Method Name: impute_missing_values Description: This method will be used to impute missing values in the dataframe Categorical to numerical \u00b6 Method Name: type_conversion Description: This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible. Imbalanced data set handling \u00b6 Method Name: remove_imbalance Description: This method will be used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques Handling columns with std deviation zero or below a threshold \u00b6 Method Name: standardize_data Description: This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1. Normalisation \u00b6 Method Name: normalize_data Description: This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1. PCA \u00b6 Method Name: pca Description: This method reduces the dimension from scaled Data which enables quick for large data files. Data Preprocessor \u00b6 Class Name DataPreprocessor Method Name impute_missing_values Method Description This method will be used to read data from a csv file or a flat file. Input parameter names self,file_name, header,names, use_cols, separator. Input Parameter Description file_name: name of the file to be read. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use Output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message Exceptions Scenarios Module Wise \u00b6 Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input","title":"Data preprocessing"},{"location":"ForDevelopers/Methodsforpreprocessing/#transformations","text":"","title":"Transformations"},{"location":"ForDevelopers/Methodsforpreprocessing/#imputing-missing-values","text":"Method Name: impute_missing_values Description: This method will be used to impute missing values in the dataframe","title":"Imputing missing values"},{"location":"ForDevelopers/Methodsforpreprocessing/#categorical-to-numerical","text":"Method Name: type_conversion Description: This method will be used to convert column datatype from numerical to categorical or vice-versa, if possible.","title":"Categorical to numerical"},{"location":"ForDevelopers/Methodsforpreprocessing/#imbalanced-data-set-handling","text":"Method Name: remove_imbalance Description: This method will be used to handle unbalanced datasets(rare classes) through oversampling/ undersampling techniques","title":"Imbalanced data set handling"},{"location":"ForDevelopers/Methodsforpreprocessing/#handling-columns-with-std-deviation-zero-or-below-a-threshold","text":"Method Name: standardize_data Description: This method will be used to standardize al the numeric variables. Where mean = 0, std dev = 1.","title":"Handling columns with std deviation zero or below a threshold"},{"location":"ForDevelopers/Methodsforpreprocessing/#normalisation","text":"Method Name: normalize_data Description: This method will be used to normalize all the numeric variables. Where min value = 0 and max value = 1.","title":"Normalisation"},{"location":"ForDevelopers/Methodsforpreprocessing/#pca","text":"Method Name: pca Description: This method reduces the dimension from scaled Data which enables quick for large data files.","title":"PCA"},{"location":"ForDevelopers/Methodsforpreprocessing/#data-preprocessor","text":"Class Name DataPreprocessor Method Name impute_missing_values Method Description This method will be used to read data from a csv file or a flat file. Input parameter names self,file_name, header,names, use_cols, separator. Input Parameter Description file_name: name of the file to be read. header: Row number(s) to be used as column names names : array-like, optional List of column names to use. If file contains no header row, then you should explicitly pass header=None . Use_cols: To load a subset of columns Separator: Delimiter to use Output A pandas Dataframe On Exception Write the exception in the log file. Raise an exception with the appropriate error message","title":"Data Preprocessor"},{"location":"ForDevelopers/Methodsforpreprocessing/#exceptions-scenarios-module-wise","text":"Step Exception Mitigation Wrong parameters passed to the methods Handle Internally Code should never give a wrong input","title":"Exceptions Scenarios Module Wise"},{"location":"ForDevelopers/MethodsforstatbasedEDA/","text":"Technical solution \u00b6 OLS \u00b6 Ordinary least squares The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values. VIF \u00b6 The variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. Correlation: Correlation is a statistical technique that can show whether and how strongly pairs of variables are related Anova test \u00b6 An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis. Basically, you're testing groups to see if there's a difference between them. Chi-square test \u00b6 Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. Z-test \u00b6 A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test. T test \u00b6 The t test is one type of inferential statistics. It is used to determine whether there is a significant difference between the means of two groups. With all inferential statistics, we assume the dependent variable fits a normal distribution Weight of evidence \u00b6 The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. F-test \u00b6 An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exceptions Scenarios \u00b6 Step Exception Mitigation Column has mixed values(Integer & number) Give proper error message Ask the user to correct the data. Not all values are numbers Handle Internally Convert categorical to numerical values Seasonality: \u00b6 Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal. Stationary Data: \u00b6 A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time","title":"Data Visualization(Statistical Analysis)"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#technical-solution","text":"","title":"Technical solution"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#ols","text":"Ordinary least squares The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values.","title":"OLS"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#vif","text":"The variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. Correlation: Correlation is a statistical technique that can show whether and how strongly pairs of variables are related","title":"VIF"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#anova-test","text":"An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis. Basically, you're testing groups to see if there's a difference between them.","title":"Anova test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#chi-square-test","text":"Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table.","title":"Chi-square test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#z-test","text":"A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test.","title":"Z-test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#t-test","text":"The t test is one type of inferential statistics. It is used to determine whether there is a significant difference between the means of two groups. With all inferential statistics, we assume the dependent variable fits a normal distribution","title":"T test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#weight-of-evidence","text":"The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable.","title":"Weight of evidence"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#f-test","text":"An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled.","title":"F-test"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#exceptions-scenarios","text":"Step Exception Mitigation Column has mixed values(Integer & number) Give proper error message Ask the user to correct the data. Not all values are numbers Handle Internally Convert categorical to numerical values","title":"Exceptions Scenarios"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#seasonality","text":"Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal.","title":"Seasonality:"},{"location":"ForDevelopers/MethodsforstatbasedEDA/#stationary-data","text":"A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time","title":"Stationary Data:"},{"location":"GettingStarted/GettingStarted/","text":"Getting Started \u00b6 Introduction \u00b6 It is an end to end automated machine learning solution where the user will only give dataset in recognizable formats and select the type of the problem, and the result will be the best performing hyper tuned machine learning model. The descriptive and graphical analysis of the data are also displayed. The user will also get privileges to choose the deployment option. (eg: AWS, GCP) Nabeel \u00b6 fghfhgfhgfh Features \u00b6 \u2022 Data sets automatically analyzed \u2022 Machine learning Modelling techniques using both classification and Regression \u2022 Extensive APIs to integrate AutoNeuro into your own tools and scripts \u2022 Evaluations of the models for easy comparison and reuse \u2022 Automated model building based on the type of problem. \u2022 Output model is hyper-parameter tuned model \u2022 Choice based automated deployment to various clouds. \u2022 Provision of user friendly interface to select input and target features","title":"Introduction & Features"},{"location":"GettingStarted/GettingStarted/#getting-started","text":"","title":"Getting Started"},{"location":"GettingStarted/GettingStarted/#introduction","text":"It is an end to end automated machine learning solution where the user will only give dataset in recognizable formats and select the type of the problem, and the result will be the best performing hyper tuned machine learning model. The descriptive and graphical analysis of the data are also displayed. The user will also get privileges to choose the deployment option. (eg: AWS, GCP)","title":"Introduction"},{"location":"GettingStarted/GettingStarted/#nabeel","text":"fghfhgfhgfh","title":"Nabeel"},{"location":"GettingStarted/GettingStarted/#features","text":"\u2022 Data sets automatically analyzed \u2022 Machine learning Modelling techniques using both classification and Regression \u2022 Extensive APIs to integrate AutoNeuro into your own tools and scripts \u2022 Evaluations of the models for easy comparison and reuse \u2022 Automated model building based on the type of problem. \u2022 Output model is hyper-parameter tuned model \u2022 Choice based automated deployment to various clouds. \u2022 Provision of user friendly interface to select input and target features","title":"Features"},{"location":"GettingStarted/How%20to%20use/","text":"How to Get AutoNeuro? \u00b6 Hardware Requirements for model training Suggested Requirment : 8 GB RAM 4 GB of Hard Disk Space Intel Core i7 Processor Minimum Requirment : 4 GB RAM 2 GB of Hard Disk Space Intel Core i7 Processor Requirements for model testing suggested Requirment : 8 GB RAM 4 GB of Hard Disk Space Intel Core i7 Processor Minimum Requirment : 4 GB RAM 2 GB of Hard Disk Space Intel Core i5 Processor Installation \u00b6 pip install autoneuro Getting HomeScreen for AutoNeuro \u00b6 \"\" here we will explain what code user has to write to start the process. For now I have written steps for running app.py\"\" If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen. Adding Data Set for Training \u00b6 To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page. On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data. Training the data \u00b6 After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Installation"},{"location":"GettingStarted/How%20to%20use/#how-to-get-autoneuro","text":"Hardware Requirements for model training Suggested Requirment : 8 GB RAM 4 GB of Hard Disk Space Intel Core i7 Processor Minimum Requirment : 4 GB RAM 2 GB of Hard Disk Space Intel Core i7 Processor Requirements for model testing suggested Requirment : 8 GB RAM 4 GB of Hard Disk Space Intel Core i7 Processor Minimum Requirment : 4 GB RAM 2 GB of Hard Disk Space Intel Core i5 Processor","title":"How to Get AutoNeuro?"},{"location":"GettingStarted/How%20to%20use/#installation","text":"pip install autoneuro","title":"Installation"},{"location":"GettingStarted/How%20to%20use/#getting-homescreen-for-autoneuro","text":"\"\" here we will explain what code user has to write to start the process. For now I have written steps for running app.py\"\" If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen.","title":"Getting HomeScreen for AutoNeuro"},{"location":"GettingStarted/How%20to%20use/#adding-data-set-for-training","text":"To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page. On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data.","title":"Adding Data Set for Training"},{"location":"GettingStarted/How%20to%20use/#training-the-data","text":"After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Training the data"},{"location":"User%27s%20Guide/Clouddeployment/","text":"Cloud Deployment \u00b6","title":"Cloud deployment"},{"location":"User%27s%20Guide/Clouddeployment/#cloud-deployment","text":"","title":"Cloud Deployment"},{"location":"User%27s%20Guide/Data%20Visualization/","text":"Data Visualization \u00b6","title":"Data Visualization"},{"location":"User%27s%20Guide/Data%20Visualization/#data-visualization","text":"","title":"Data Visualization"},{"location":"User%27s%20Guide/LoggingHelp/","text":"Logging Help \u00b6","title":"Logging Help"},{"location":"User%27s%20Guide/LoggingHelp/#logging-help","text":"","title":"Logging Help"},{"location":"User%27s%20Guide/ModelTraining/","text":"Model Training \u00b6 On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data. Training the data \u00b6 After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Model Training"},{"location":"User%27s%20Guide/ModelTraining/#model-training","text":"On this page you will be able to view detailed analysis of your training data. You will get information regarding missing values, columns with zero std deviation and if there is any duplicate column. If you have reached this page then you have successfully loaded the data for training. Now it\u2019s time to train the data.","title":"Model Training"},{"location":"User%27s%20Guide/ModelTraining/#training-the-data","text":"After successfully uploading the data go to \u201cChoose a Problem Type\u201d drop down and select the type of your Problem. You can select from Regression and Classification. AutoNeuro will automatically perform feature engineering and train your data with the highest optimization. After selecting problem type, if you want to drop any unwanted columns then you can select that from the \u201cDrop Unwanted Columns\u201d. After dropping, the final step is to select the Target Column (dependent column which you want to predict) and click \u201cSubmit\u201d.","title":"Training the data"},{"location":"User%27s%20Guide/Predictions/","text":"Predictions \u00b6 Once you have successfully trained the data you will be able to make predictions. Press \u2018Try Prediction\u2019 button at the bottom of the report for making Predictions.","title":"Predictions"},{"location":"User%27s%20Guide/Predictions/#predictions","text":"Once you have successfully trained the data you will be able to make predictions. Press \u2018Try Prediction\u2019 button at the bottom of the report for making Predictions.","title":"Predictions"},{"location":"User%27s%20Guide/Troubleshooting/","text":"Troubleshooting \u00b6","title":"Troubleshooting"},{"location":"User%27s%20Guide/Troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"User%27s%20Guide/Uploading%20Data/","text":"Uploading Data \u00b6 The process for Uploading Data Are as follows: Getting HomeScreen for AutoNeuro \u00b6 Note Here we will explain what code user has to write to start the process. For now I have written steps for running app.py If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen. Adding Data Set for Training \u00b6 To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page.","title":"Uploading Data"},{"location":"User%27s%20Guide/Uploading%20Data/#uploading-data","text":"The process for Uploading Data Are as follows:","title":"Uploading Data"},{"location":"User%27s%20Guide/Uploading%20Data/#getting-homescreen-for-autoneuro","text":"Note Here we will explain what code user has to write to start the process. For now I have written steps for running app.py If you have successfully installed AutoNeuro then: Go to application\\app.py and run Open your web browser and go to localhost:5000 On Successful execution of steps you will see this screen.","title":"Getting HomeScreen for AutoNeuro"},{"location":"User%27s%20Guide/Uploading%20Data/#adding-data-set-for-training","text":"To add training dataset: Click on Choose a data sources drop down: Select the type of the file you are uploading. You can upload CSV, Excel, HTML or Text files. Click on Choose file and select your training dataset file. Once you select the dataset it will be validated and on successful validation you would be able to upload data for training. On clicking upload your data will be uploaded and you will be land to the following page.","title":"Adding Data Set for Training"}]}